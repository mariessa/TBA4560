Automatically generated by Mendeley Desktop 1.17.9
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Kartverket2014,
author = {Kartverket},
file = {:Users/mathilde/skole/papers/produktspesifikasjoner{\_}Digitale ortofoto{\_}v5{\_}ortofoto-spesifikasjon-v4{\_}5-2013{\_}.pdf:pdf},
title = {{Product Specification version 4.5 - Aerial Photo Norway}},
url = {http://kartverket.no/Documents/Standard/SOSI kap3 Produktspesifikasjoner/Ortofoto/ProduktspesifikasjonOrtofotoversjon45-20130110.pdf},
year = {2014}
}
@article{Ofli2016,
abstract = {Aerial imagery captured via unmanned aerial vehicles (UAVs) is playing an increasingly important role in disaster response. Unlike satellite imagery, aerial imagery can be captured and processed within hours rather than days. In addition, the spatial resolution of aerial imagery is an order of magnitude higher than the imagery produced by the most sophisticated commercial satellites today. Both the United States Federal Emergency Management Agency (FEMA) and the European Commission's Joint Research Center ( JRC) have noted that aerial imagery will inevitably present a big data challenge. The purpose of this article is to get ahead of this future challenge by proposing a hybrid crowdsourcing and real-time machine learning solution to rapidly process large volumes of aerial data for disaster response in a time-sensitive manner. Crowdsourcing can be used to annotate features of interest in aerial images (such as damaged shelters and roads blocked by debris). These human-annotated features can then be used to train a supervised machine learning system to learn to recognize such features in new unseen images. In this article, we describe how this hybrid solution for image analysis can be imple- mented as a module (i.e., Aerial Clicker) to extend an existing platform called Artificial Intelligence for Disaster Response (AIDR), which has already been deployed to classify microblog messages during disasters using its Text Clicker module and in response to Cyclone Pam, a category 5 cyclone that devastated Vanuatu in March 2015. The hybrid solution we present can be applied to both aerial and satellite imagery and has applications beyond disaster response such as wildlife protection, human rights, and archeological exploration. As a proof of concept, we recently piloted this solution using very high-resolution aerial photographs of a wildlife reserve in Namibia to support rangers with their wildlife conservation efforts (SAVMAP project, http://lasig.epfl.ch/savmap). The results suggest that the platform we have developed to combine crowdsourcing and machine learning to make sense of large volumes of aerial images can be used for disaster response.},
author = {Ofli, Ferda and Meier, Patrick and Imran, Muhammad and Castillo, Carlos and Tuia, Devis and Rey, Nicolas and Briant, Julien and Millet, Pauline and Reinhard, Friedrich and Parkan, Matthew and Joost, St{\'{e}}phane},
doi = {10.1089/big.2014.0064},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/ofli{\_}meier{\_}imran{\_}castillo{\_}al{\_}2016{\_}combining{\_}human{\_}computing{\_}machine{\_}learning.pdf:pdf},
issn = {2167-6461},
journal = {Big Data},
keywords = {big data analytics,crowdsourcing,machine learning,remote sensing,uav},
number = {1},
pages = {47--59},
title = {{Combining Human Computing and Machine Learning to Make Sense of Big (Aerial) Data for Disaster Response}},
url = {http://online.liebertpub.com/doi/10.1089/big.2014.0064},
volume = {4},
year = {2016}
}
@book{Goodfellow2016,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
publisher = {MIT Press},
title = {{Deep Learning}},
year = {2016}
}
@misc{Kendall2014,
author = {Kendall, Alex},
booktitle = {GitHub},
file = {:Users/mathilde/Library/Application Support/Mendeley Desktop/Downloaded/Kendall - 2014 - Caffe SegNet.html:html},
howpublished = {https://github.com/alexgkendall/caffe-segnet. Date Accessed: 2017-05-12},
title = {{Caffe SegNet}},
url = {https://github.com/alexgkendall/caffe-segnet},
urldate = {2017-05-12},
year = {2014}
}
@article{Kampffmeyer2016,
abstract = {We propose a deep convolutional neural network (CNN) for object detection and land cover mapping in remote sens-ing images, with a focus on urban areas. In remote sensing, class imbalance represents often a problem for tasks like land cover mapping, as small objects get less prioritised in an effort to achieve the best overall accuracy. We propose a novel approach to achieve high overall accuracy, while still achieving good accuracy for small objects. Quantify-ing the uncertainty on a pixel scale is another challenge in remote sensing, especially when using CNNs. In this paper we use recent advances in measuring uncertainty for CNNs and evaluate their quality both qualitatively and quanti-tatively in a remote sensing context. We demonstrate our ideas on different deep architectures including patch-based and so-called pixel-to-pixel approaches, as well as their combination, by classifying each pixel in a set of aerial im-ages covering Vaihingen, Germany. The results show that we obtain an overall classification accuracy of 87{\%}. The corresponding F1-score for the small object class " car " is 80.6{\%}, which is higher than state-of-the art for this dataset.},
author = {Kampffmeyer, Michael and Jenssen, Robert},
doi = {10.1109/CVPRW.2016.90},
file = {:Users/mathilde/Library/Application Support/Mendeley Desktop/Downloaded/Kampffmeyer, Jenssen - 2016 - Detection of small objects, land cover mapping and modelling of uncertainty in urban remote sensing images.pdf:pdf},
isbn = {9781509014378},
pages = {680--688},
title = {{Detection of small objects, land cover mapping and modelling of uncertainty in urban remote sensing images using deep convolutional neural networks}},
year = {2016}
}
@article{Arun2013,
abstract = {Automatic feature extraction domain has witnessed the application of many intelligent methodologies over past decade; however detection accuracy of these approaches were limited as object geometry and contextual knowledge were not given enough consideration. In this paper, we propose a frame work for accurate detection of features along with automatic interpolation, and interpretation by modeling feature shape as well as contextual knowledge using advanced techniques such as SVRF, Cellular Neural Network, Core set, and MACA. Developed methodology has been compared with contemporary methods using different statistical measures. Investigations over various satellite images revealed that considerable success was achieved with the CNN approach. CNN has been effective in modeling different complex features effectively and complexity of the approach has been considerably reduced using corset optimization. The system has dynamically used spectral and spatial information for 1 Email : arunpv2601@gmail.com representing contextual knowledge using CNN-prolog approach. System has been also proved to be effective in providing intelligent interpolation and interpretation of random features.},
annote = {NULL},
author = {Arun, P. V.},
doi = {10.1080/10106049.2013.826738},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/An intelligent...pdf:pdf},
issn = {1010-6049},
journal = {Geocarto International},
pages = {130806010607002},
title = {{An intelligent approach towards automatic shape modeling and object extraction from satellite images using cellular automata based algorithm}},
url = {http://www.tandfonline.com/doi/abs/10.1080/10106049.2013.826738},
year = {2013}
}
@book{Bengio2009,
author = {Bengio, Yoshua},
edition = {Vol. 2},
publisher = {Universite de Montreal},
title = {{Learning deep architectures for AI}},
year = {2009}
}
@misc{Sukhova,
author = {Sukhova, Maria},
file = {:Users/mathilde/Library/Application Support/Mendeley Desktop/Downloaded/Sukhova - 2016 - Five Signs That You Need to Reengineer Your Software {\_} Software Development News, Events, Thoughts from Auriga.htm:htm},
howpublished = {https://auriga.com/blog/five-signs-that-you-need-to-reengineer-your-software/. Date Accessed: 2016-11-23.},
title = {{Five Signs That You Need to Reengineer Your Software {\_} Software Development News, Events, Thoughts from Auriga}},
url = {https://auriga.com/blog/five-signs-that-you-need-to-reengineer-your-software/},
urldate = {2016-11-23},
year = {2016}
}
@article{Badrinarayanan2015,
abstract = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1]. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps.We compare our proposed architecture with the fully convolutional network [2] architecture and its variants. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. The design of SegNet was primarily motivated by road scene understanding applications. Hence, it is efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than competing architectures and can be trained end-to-end using stochastic gradient descent without complex training protocols.We also benchmark the performance of SegNet on Pascal VOC12 salient object segmentation and the recent SUN RGB-D indoor scene understanding challenge. These quantitative assessments show that SegNet provides competitive performance although it is significantly smaller than other architectures.We also provide a Caffe implementation of SegNet and a webdemo at http://mi.eng.cam.ac.uk/projects/segnet/.},
annote = {SegNet},
archivePrefix = {arXiv},
arxivId = {1505.0729},
author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
doi = {10.1103/PhysRevX.5.041024},
eprint = {1505.0729},
file = {:Users/mathilde/Library/Application Support/Mendeley Desktop/Downloaded/Badrinarayanan, Kendall, Cipolla - 2015 - SegNet A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation.pdf:pdf},
issn = {21603308},
journal = {Cvpr 2015},
keywords = {Decoder,Deep Convolutional Neural Networks,Encoder,Pooling,Semantic Pixel-Wise Segmentation,Upsampling},
pages = {5},
pmid = {28060704},
title = {{SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation}},
url = {http://arxiv.org/abs/1505.0729{\%}5Cnhttp://mi.eng.cam.ac.uk/projects/segnet/},
year = {2015}
}
@article{Schmidhuber2015,
abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning {\&} evolutionary computation, and indirect search for short programs encoding deep and large networks.},
archivePrefix = {arXiv},
arxivId = {1404.7828},
author = {Schmidhuber, Jorgen},
doi = {10.1016/j.neunet.2014.09.003},
eprint = {1404.7828},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/1404.7828.pdf:pdf},
isbn = {0893-6080},
issn = {18792782},
journal = {Neural Networks},
keywords = {Deep learning,Evolutionary computation,Reinforcement learning,Supervised learning,Unsupervised learning},
pages = {85--117},
pmid = {25462637},
title = {{Deep Learning in neural networks: An overview}},
volume = {61},
year = {2015}
}
@article{Romero2015,
abstract = {This paper introduces the use of single-layer and deep convolutional networks for remote sensing data analysis. Direct application to multi- and hyperspectral imagery of super- vised (shallow or deep) convolutional networks is very challenging given the high input data dimensionality and the relatively small amount of available labeled data. Therefore, we propose the use of greedy layerwise unsupervised pretraining coupled with a highly efficient algorithm for unsupervised learning of sparse features. The algorithm is rooted on sparse representations and enforces both population and lifetime sparsity of the extracted features, simultaneously.We successfully illustrate the expressive power of the extracted representations in several scenarios: classification of aerial scenes, as well as land-use classification in very high resolution or land-cover classification from multi- and hyperspec- tral images. The proposed algorithmclearly outperforms standard principal component analysis (PCA) and its kernel counterpart (kPCA), as well as current state-of-the-art algorithms of aerial classification, while being extremely computationally efficient at learning representations of data. Results show that single-layer convolutional networks can extract powerful discriminative fea- tures only when the receptive field accounts for neighboring pixels and are preferred when the classification requires high resolution and detailed results. However, deep architectures significantly outperform single-layer variants, capturing increasing levels of abstraction and complexity throughout the feature hierarchy.},
annote = {cited 22},
author = {Romero, Adriana and Gatta, Carlo and Camps-valls, Gustau and Member, Senior},
doi = {10.1109/TGRS.2015.2478379.},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/Unsupervised Deep Feature Extraction for Remote Sensing Image Classification.pdf:pdf},
issn = {0196-2892},
journal = {IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 1 Unsupervised},
number = {3},
pages = {1--14},
title = {{Unsupervised Deep Feature Extraction for Remote Sensing Image Classification}},
volume = {54},
year = {2015}
}
@article{Noh2015,
abstract = {We propose a novel semantic segmentation algorithm by learning a deconvolution network. We learn the network on top of the convolutional layers adopted from VGG 16-layer net. The deconvolution network is composed of deconvolution and unpooling layers, which identify pixel-wise class labels and predict segmentation masks. We apply the trained network to each proposal in an input image, and construct the final semantic segmentation map by combining the results from all proposals in a simple manner. The proposed algorithm mitigates the limitations of the existing methods based on fully convolutional networks by integrating deep deconvolution network and proposal-wise prediction; our segmentation method typically identifies detailed structures and handles objects in multiple scales naturally. Our network demonstrates outstanding performance in PASCAL VOC 2012 dataset, and we achieve the best accuracy (72.5{\%}) among the methods trained with no external data through ensemble with the fully convolutional network.},
annote = {Mention in class stanford},
archivePrefix = {arXiv},
arxivId = {1505.04366},
author = {Noh, Hyeonwoo and Hong, Seunghoon and Han, Bohyung},
doi = {10.1109/ICCV.2015.178},
eprint = {1505.04366},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/1505.04366v1.pdf:pdf},
isbn = {978-1-4673-8391-2},
issn = {15505499},
title = {{Learning Deconvolution Network for Semantic Segmentation}},
url = {http://arxiv.org/abs/1505.04366},
volume = {1},
year = {2015}
}
@article{Duchi2011,
abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
archivePrefix = {arXiv},
arxivId = {arXiv:1103.4296v1},
author = {Duchi, John and Hazan, Elad and Singer, Yoram},
doi = {10.1109/CDC.2012.6426698},
eprint = {arXiv:1103.4296v1},
file = {:Users/mathilde/skole/papers/duchi11a.pdf:pdf},
isbn = {9780982252925},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {adaptivity,online learning,stochastic convex optimization,subgradient methods},
pages = {2121--2159},
pmid = {2868127},
title = {{Adaptive Subgradient Methods for Online Learning and Stochastic Optimization}},
url = {http://jmlr.org/papers/v12/duchi11a.html},
volume = {12},
year = {2011}
}
@misc{Canada2015,
author = {Canada, Natural Resources},
file = {:Users/mathilde/skole/papers/Geometric Distortion in Imagery {\_} Natural Resources Canada.htm:htm},
howpublished = {http://www.nrcan.gc.ca/node/9401},
title = {{Geometric Distortion in Imagery}},
url = {http://www.nrcan.gc.ca/node/9401},
urldate = {2017-05-21},
year = {2015}
}
@misc{Li2015,
author = {Li, Fei-fei and Karpathy, Andrej},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/winter1516{\_}lecture2.pdf:pdf},
howpublished = {http://cs231n.stanford.edu/slides/winter1516{\_}lecture2.pdf. Date Accessed: Date Accessed: 14.03.2017},
pages = {1--32},
title = {{Lecture 2 : Image Classification pipeline}},
url = {http://cs231n.stanford.edu/slides/winter1516{\_}lecture2.pdf},
urldate = {2016-11-26},
year = {2015}
}
@misc{AndrejKarpathy2015,
abstract = {Convolutional Neural Networks are very similar to ordinary Neural Networks from the previous chapter: they are made up of neurons that have learnable weights and biases. Each neuron receives some inputs, performs a dot product and optionally follows it with a non-linearity. The whole network still expresses a single differentiable score function: from the raw image pixels on one end to class scores at the other. And they still have a loss function (e.g. SVM/Softmax) on the last (fully-connected) layer and all the tips/tricks we developed for learning regular Neural Networks still apply. So what does change? ConvNet architectures make the explicit assumption that the inputs are images, which allows us to encode certain properties into the architecture. These then make the forward function more efficient to implement and vastly reduce the amount of parameters in the network.},
author = {Karpathy, Andrej},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/CS231n Convolutional Neural Networks for Visual Recognition.htm:htm},
howpublished = {http://cs231n.github.io/convolutional-networks/. Date Accessed: 2016-09-26.},
isbn = {8978358160},
pages = {1--5},
title = {{CS231n : Convolutional Neural Networks for Visual Recognition}},
url = {http://cs231n.github.io/convolutional-networks/},
urldate = {2016-09-26},
year = {2015}
}
@misc{GISGeography2015,
author = {{GIS Geography}},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/Image Classification Techniques in Remote Sensing.htm:htm},
howpublished = {http://gisgeography.com/image-classification-techniques-remote-sensing/. Date Accessed: 2016-12-18.},
title = {{Image Classification Techniques in Remote Sensing}},
url = {http://gisgeography.com/image-classification-techniques-remote-sensing/},
urldate = {2016-12-18},
year = {2015}
}
@article{Kell2016,
author = {Kell, David},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/Gyana{\_}White{\_}Paper{\_}Aerial{\_}Images{\_}Final.pdf:pdf},
number = {January},
title = {{How not to do Deep Learning on Aerial Images}},
year = {2016}
}
@misc{Valigi16,
author = {Valigi, Nicolo},
file = {:Users/mathilde/skole/papers/A review of deep learning models for semantic segmentation - Nicol{\`{o}} Valigi.htm:htm},
howpublished = {http://nicolovaligi.com/deep-learning-models-semantic-segmentation.html},
title = {{A review of deep learning models for semantic segmentation}},
url = {http://nicolovaligi.com/deep-learning-models-semantic-segmentation.html},
urldate = {2017-05-21},
year = {2016}
}
@misc{Deshpande,
author = {Deshpande, Adit},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/A Beginner's Guide To Understanding Convolutional Neural Networks – Adit Deshpande – CS Undergrad at UCLA ('19).htm:htm},
howpublished = {https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/. Date Accessed: 2016-11-29.},
title = {{A Beginner's Guide To Understanding Convolutional Neural Networks Part 2}},
url = {https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/},
urldate = {2016-11-29},
year = {2016}
}
@article{Ruder2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1609.04747v1},
author = {Ruder, Sebastian},
eprint = {arXiv:1609.04747v1},
file = {:Users/mathilde/skole/papers/1609.04747.pdf:pdf},
pages = {1--12},
title = {{An overview of gradient descent optimization}},
year = {2016}
}
@article{Mistry2013,
author = {Mistry, Priya},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/projectReport Marianne.pdf:pdf},
pages = {1--35},
title = {{Project Report}},
year = {2013}
}
@article{Tang2015,
annote = {NULL},
author = {Tang, Shijian},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/CS231n{\_}final{\_}writeup{\_}sjtang.pdf:pdf},
pages = {8},
title = {{Object Detection based on Convolutional Neural Network}},
year = {2015}
}
@article{Maire2014,
annote = {NULL},
author = {Maire, Frederic and Mejias, Luis and Hodgson, Amanda},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/Automatic rec of aerial with conv nn.pdf:pdf},
isbn = {9781479954094},
journal = {Digital Image Computing: Techniques and Applications},
title = {{A Convolutional Neural Network for Automatic Analysis of Aerial Imagery}},
year = {2014}
}
@article{Simonyan2015,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
annote = {VGGNet},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
doi = {10.1016/j.infsof.2008.09.005},
eprint = {1409.1556},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/1409.1556.pdf:pdf},
isbn = {9781450341448},
issn = {09505849},
journal = {International Conference on Learning Representations},
pages = {1--14},
pmid = {16873662},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {http://arxiv.org/abs/1409.1556},
year = {2015}
}
@article{Zheng2015,
abstract = {Combination of a deep neural network with conditional random fields. The conditional random fields are trained with BP, using BP trhough time. Pixel-level labelling tasks, such as semantic segmenta- tion, play a central role in image understanding. Recent ap- proaches have attempted to harness the capabilities of deep learning techniques for image recognition to tackle pixel- level labelling tasks. One central issue in this methodology is the limited capacity of deep learning techniques to de- lineate visual objects. To solve this problem, we introduce a new form of convolutional neural network that combines the strengths of Convolutional Neural Networks (CNNs) and Conditional Random Fields (CRFs)-based probabilis- tic graphical modelling. To this end, we formulate Con- ditional Random Fields with Gaussian pairwise potentials and mean-field approximate inference as Recurrent Neural Networks. This network, called CRF-RNN, is then plugged in as a part of a CNN to obtain a deep network that has desirable properties of both CNNs and CRFs. Importantly, our system fully integrates CRF modelling with CNNs, mak- ing it possible to train the whole deep network end-to-end with the usual back-propagation algorithm, avoiding offline post-processing methods for object delineation. We apply the proposed method to the problem of seman- tic image segmentation, obtaining top results on the chal- lenging Pascal VOC 2012 segmentation benchmark.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {arXiv:1502.03240v1},
author = {Zheng, Shuai and Jayasumana, Sadeep and Romera-Paredes, B. and Vineet, Vibhav and Su, Zhizhong and Du, Dalong and Huang, Chang and Torr, Philip H S},
doi = {10.1109/ICCV.2015.179},
eprint = {arXiv:1502.03240v1},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/1502.03240.pdf:pdf},
isbn = {978-1-4673-8391-2},
issn = {15505499},
journal = {Iccv},
keywords = {deep learning,error back-propagation,neural network,semantic segmentation},
pages = {1529--1537},
title = {{Conditional Random Fields as Recurrent Neural Networks}},
year = {2015}
}
@article{Gunduz2006,
author = {G{\"{u}}nd{\"{u}}z, Mesut and Yildiz, Ferruh and Onat, Ayşe},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/Feature Extraction from Aerial Photos .pdf:pdf},
journal = {Engineering and Technology},
keywords = {aerial photos,vectorization,vectorized drawing},
number = {2},
pages = {74--77},
title = {{Feature Extraction from Aerial Photos}},
volume = {2},
year = {2006}
}
@article{He2015a,
annote = {Segnet refers to this paper for technique for intializing weights

- They also show that xavier worsk good?},
archivePrefix = {arXiv},
arxivId = {1502.01852},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.1109/ICCV.2015.123},
eprint = {1502.01852},
file = {:Users/mathilde/skole/papers/1502.01852.pdf:pdf},
isbn = {978-1-4673-8391-2},
issn = {15505499},
journal = {CoRR},
title = {{Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification}},
url = {http://arxiv.org/abs/1502.01852},
volume = {abs/1502.0},
year = {2015}
}
@article{Hosang2014,
abstract = {Current top performing Pascal VOC object detectors employ detection proposals to guide the search for objects thereby avoiding exhaustive sliding window search across images. Despite the popularity of detection proposals, it is unclear which trade-offs are made when using them during object detection. We provide an in depth analysis of ten object proposal methods along with four baselines regarding ground truth annotation recall (on Pascal VOC 2007 and ImageNet 2013), repeatability, and impact on DPM detector performance. Our findings show common weaknesses of existing methods, and provide insights to choose the most adequate method for different settings.},
annote = {Compares region proposals methods, concludes that edge box is best

this paper was referred to in stanford class as part of theobject detection history},
archivePrefix = {arXiv},
arxivId = {1406.6962},
author = {Hosang, Jan and Benenson, Rodrigo and Schiele, Bernt},
doi = {10.5244/C.28.24},
eprint = {1406.6962},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/2014{\_}bmvc{\_}selective{\_}search{\_}with{\_}supplementary{\_}material.pdf:pdf},
isbn = {1-901725-52-9},
journal = {Bmvc},
pages = {1--25},
title = {{How good are detection proposals, really?}},
url = {http://arxiv.org/abs/1406.6962},
year = {2014}
}
@article{Sherrah2016,
abstract = {The trend towards higher resolution remote sensing imagery facilitates a transition from land-use classification to object-level scene understanding. Rather than relying purely on spectral content, appearance-based image features come into play. In this work, deep convolutional neural networks (CNNs) are applied to semantic labelling of high-resolution remote sensing data. Recent advances in fully convolutional networks (FCNs) are adapted to overhead data and shown to be as effective as in other domains. A full-resolution labelling is inferred using a deep FCN with no downsampling, obviating the need for deconvolution or interpolation. To make better use of image features, a pre-trained CNN is fine-tuned on remote sensing data in a hybrid network context, resulting in superior results compared to a network trained from scratch. The proposed approach is applied to the problem of labelling high-resolution aerial imagery, where fine boundary detail is important. The dense labelling yields state-of-the-art accuracy for the ISPRS Vaihingen and Potsdam benchmark data sets.},
annote = {cited by 7},
archivePrefix = {arXiv},
arxivId = {1606.02585},
author = {Sherrah, Jamie},
eprint = {1606.02585},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/1606.02585v1.pdf:pdf},
journal = {arXiv},
pages = {1--22},
title = {{Fully Convolutional Networks for Dense Semantic Labelling of High-Resolution Aerial Imagery}},
url = {http://arxiv.org/abs/1606.02585},
year = {2016}
}
@article{Ban2011,
author = {Ban, Yifang},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/Recap fag fra Marianne.pdf:pdf},
pages = {1--4},
title = {{Lecture 1 Lecture 2 Lecture 3}},
year = {2011}
}
@article{Krizhevsky2012a,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0 {\%} which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3{\%}, compared to 26.2 {\%} achieved by the second-best entry. 1},
annote = {AlexNet paper!!},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Krizhevsky, Alex and Sutskever, Ilya and {Geoffrey E.}, Hinton},
doi = {10.1109/5.726791},
eprint = {1102.0183},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/imagenet{\_}classification{\_}with{\_}deep{\_}convolutional.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems 25 (NIPS2012)},
pages = {1--9},
pmid = {15823584},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
url = {https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
year = {2012}
}
@article{Chen2014,
abstract = {Detecting small objects such as vehicles in satellite images is a difficult problem. Many features (such as histogram of oriented gradient, local binary pattern, scale-invariant feature transform, etc.) have been used to improve the performance of object detection, but mostly in simple environments such as those on roads. Kembhavi et al. proposed that no satisfactory accuracy has been achieved in complex environments such as the City of San Francisco. Deep convolutional neural networks (DNNs) can learn rich features from the training data automatically and has achieved state-of-the-art performance in many image classification databases. Though the DNN has shown robustness to distortion, it only extracts features of the same scale, and hence is insufficient to tolerate large-scale variance of object. In this letter, we present a hybrid DNN (HDNN), by dividing the maps of the last convolutional layer and the maxpooling layer of DNN into multiple blocks of variable receptive field sizes or max-pooling field sizes, to enable the HDNN to extract variable-scale features. Comparative experimental results indicate that our proposed HDNN significantly outperforms the traditional DNN on vehicle detection.},
annote = {NULL},
author = {Chen, X Y and Xiang, S M and Liu, C L and Pan, C H},
doi = {10.1109/Lgrs.2014.2309695},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/Vehicle Detection in Satellite Images by Hybrid Deep Convolutional Neural Networks.pdf:pdf},
isbn = {1545-598x},
issn = {1545-598X},
journal = {Ieee Geoscience and Remote Sensing Letters},
keywords = {classification,deep convolutional neural networks (dnns),hybrid dnns (hdnns),recognition,remote sensing,vehicle detection},
number = {10},
pages = {1797--1801},
title = {{Vehicle Detection in Satellite Images by Hybrid Deep Convolutional Neural Networks}},
volume = {11},
year = {2014}
}
@misc{Globa,
annote = {NULL},
author = {{Global Forest}},
file = {:Users/mathilde/Library/Application Support/Mendeley Desktop/Downloaded/Global Forest - Unknown - How Does Satellite Internet Work.htm:htm},
howpublished = {http://www.groundcontrol.com/How{\_}Does{\_}Satellite{\_}Internet{\_}Work.htm. Date Accessed: 2016-11-19.},
keywords = {How Does Satellite},
title = {{How Does Satellite Internet Work?}},
url = {http://www.groundcontrol.com/How{\_}Does{\_}Satellite{\_}Internet{\_}Work.htm},
urldate = {2016-11-19}
}
@article{Penatti2015,
annote = {Viser til at pretraining fungerer selv for overhead images selv om pretraininger er gjort p{\aa} multimedia images},
author = {Penatti, A B and Nogueira, Keiller and Santos, Jefersson A},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/Penatti{\_}Do{\_}Deep{\_}Features{\_}2015{\_}CVPR{\_}paper.pdf:pdf},
isbn = {9781467367592},
pages = {44--51},
title = {{Do Deep Features Generalize from Everyday Objects to Remote Sensing and Aerial Scenes Domains ?}},
year = {2015}
}
@article{Quackenbush2004,
author = {Quackenbush, Lindi J},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/A Review of Techniques for Extracting Linear Features from Imagery.pdf:pdf},
keywords = {Photogrammetric Engineering {\&} Remote Sensing
Vol.},
number = {December},
title = {{A Review of Techniques for Extracting Linear Features from Imagery}},
year = {2004}
}
@article{orstavik17,
author = {{\O}rstavik, Mathilde and Midtb{\o}, Terje},
journal = {Kart og Plan},
number = {To be published},
title = {{A New Era for Feature Extraction in Remotely Sensed Images by The Use of Machine Learning}},
year = {2017}
}
@misc{Planet,
author = {Planet},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/Planet — Products.htm:htm},
howpublished = {https://www.planet.com/products/{\#}satellite-imagery. Date Accessed: 2016-12-17.},
title = {{Planet — Products}},
url = {https://www.planet.com/products/{\#}satellite-imagery},
urldate = {2016-12-17}
}
@article{Zitnick2014,
abstract = {The use of object proposals is an effective recent approach for increasing the computational efficiency of object detection. We propose a novel method for generating object bounding box proposals using edges. Edges provide a sparse yet informative representation of an image. Our main observation is that the number of contours that are wholly contained in a bounding box is indicative of the likelihood of the box containing an object. We propose a simple box objectness score that measures the number of edges that exist in the box minus those that are members of contours that overlap the box's boundary. Using efficient data structures, millions of candidate boxes can be evaluated in a fraction of a second, returning a ranked set of a few thousand top-scoring proposals. Using standard metrics, we show results that are significantly more accurate than the current state-of-the-art while being faster to compute. In particular, given just 1000 proposals we achieve over 96{\%} object recall at overlap threshold of 0.5 and over 75{\%} recall at the more challenging overlap of 0.7. Our approach runs in 0.25 seconds and we additionally demonstrate a near real-time variant with only minor loss in accuracy.},
annote = {Best method for regoional proposals before R-CNN},
archivePrefix = {arXiv},
arxivId = {1411.4038},
author = {Zitnick, C. Lawrence and Doll{\'{a}}r, Piotr},
doi = {10.1007/978-3-319-10602-1_26},
eprint = {1411.4038},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/EdgeBoxes{\_}ECCV2014.pdf:pdf},
isbn = {978-3-319-10601-4},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {edge detection,object detection,object proposals},
number = {PART 5},
pages = {391--405},
title = {{Edge boxes: Locating object proposals from edges}},
volume = {8693 LNCS},
year = {2014}
}
@article{Ren2015,
abstract = {Most object detectors contain two important components: a feature extractor and an object classifier. The feature extractor has rapidly evolved with significant research efforts leading to better deep ConvNet architectures. The object classifier, however, has not received much attention and most state-of-the-art systems (like R-CNN) use simple multi-layer perceptrons. This paper demonstrates that carefully designing deep networks for object classification is just as important. We take inspiration from traditional object classifiers, such as DPM, and experiment with deep networks that have part-like filters and reason over latent variables. We discover that on pre-trained convolutional feature maps, even randomly initialized deep classifiers produce excellent results, while the improvement due to fine-tuning is secondary; on HOG features, deep classifiers outperform DPMs and produce the best HOG-only results without external data. We believe these findings provide new insight for developing object detection systems. Our framework, called Networks on Convolutional feature maps (NoC), achieves outstanding results on the PASCAL VOC 2007 (73.3{\%} mAP) and 2012 (68.8{\%} mAP) benchmarks.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1504.06066},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Zhang, Xiangyu and Sun, Jian},
doi = {10.1109/TPAMI.2016.2601099},
eprint = {1504.06066},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/Object Detection Networks on Convolutional Feature Maps.pdf:pdf},
isbn = {9781467369640},
issn = {0162-8828},
journal = {Cvpr},
number = {c},
pages = {1265--1274},
title = {{Object Detection Networks on Convolutional Feature Maps}},
url = {http://arxiv.org/abs/1504.06066},
volume = {8828},
year = {2015}
}
@article{Eigen2014,
abstract = {In this paper we address three different computer vision tasks using a single basic architecture: depth prediction, surface normal estimation, and semantic labeling. We use a multiscale convolutional network that is able to adapt easily to each task using only small modifications, regressing from the input image to the output map directly. Our method progressively refines predictions using a sequence of scales, and captures many image details without any superpixels or low-level segmentation. We achieve state-of-the-art performance on benchmarks for all three tasks.},
annote = {SegNet referer til denne n{\aa}r den snakker om class balancing},
archivePrefix = {arXiv},
arxivId = {1411.4734},
author = {Eigen, David and Fergus, Rob},
doi = {10.1109/ICCV.2015.304},
eprint = {1411.4734},
file = {:Users/mathilde/skole/masteroppgave/1411.4734.pdf:pdf},
isbn = {978-1-4673-8391-2},
journal = {ICCV},
title = {{Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-Scale Convolutional Architecture}},
url = {http://arxiv.org/abs/1411.4734},
year = {2014}
}
@article{Kingma2015,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy Lei},
eprint = {1412.6980},
file = {:Users/mathilde/Library/Application Support/Mendeley Desktop/Downloaded/Kingma, Ba - 2015 - Adam a Method for Stochastic Optimization.pdf:pdf},
journal = {International Conference on Learning Representations 2015},
pages = {1--15},
title = {{Adam: a Method for Stochastic Optimization}},
year = {2015}
}
@article{Szegedy2014,
annote = {GoogLeNet},
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew and Hill, Chapel and Arbor, Ann},
doi = {10.1109/CVPR.2015.7298594},
eprint = {1409.4842},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/GoogLeNet.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
pmid = {24920543},
title = {{Going Deeper with Convolutions}},
year = {2014}
}
@article{Wang2015,
abstract = {Extracting road networks from very-high-resolution (VHR) aerial and satellite imagery has been a long-standing problem. In this article, a neural-dynamic tracking framework is proposed to extract road networks based on deep convolutional neural networks (DNN) and a finite state machine (FSM). Inspired by autonomous mobile systems, the authors train a DNN to recognize the pattern of input data, which is an image patch extracted in a detection window centred at the current location of the tracker. The pattern is predefined according to the environment and associated with the states in the FSM. A vector-guided sampling method is proposed to generate the training data set for the DNN, which extracts massive image-direction pairs from the imagery and existing vector road maps. In the tracking procedure, the size of the detection window is determined by a fusion strategy and the extracted image patches represent the orientation features of the road (local environment) that can be recognized by the trained DNN. ...},
annote = {NULL},
author = {Wang, Jun and Song, Jingwei and Chen, Mingquan and Yang, Zhi},
doi = {10.1080/01431161.2015.1054049},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/Road network extraction a neural dynamic framework based on deep learning and a finite state machine.pdf:pdf},
issn = {0143-1161},
journal = {International Journal of Remote Sensing},
number = {12},
pages = {3144--3169},
publisher = {Taylor {\&} Francis},
title = {{Road network extraction: a neural-dynamic framework based on deep learning and a finite state machine}},
url = {http://www.tandfonline.com/doi/abs/10.1080/01431161.2015.1054049},
volume = {36},
year = {2015}
}
@book{Nielsen2015,
author = {Nielsen, Michael A.},
publisher = {Determination Press},
title = {{Nural Networks and Deep Learning}},
year = {2015}
}
@article{Horak2011,
author = {Horak, Zdenek and Kudelka, Milos and Snasel, Vaclav and Vozenilek, Vit},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/Orthophoto Map Feature Extraction Based on Neural Networks.pdf:pdf},
isbn = {9788024823911},
pages = {216--225},
title = {{Orthophoto Map Feature Extraction Based on Orthophoto Map Feature Extraction Based on Neural Networks Neural Networks}},
year = {2011}
}
@misc{Mason,
author = {Mason, Matthew},
file = {:Users/mathilde/Library/Application Support/Mendeley Desktop/Downloaded/Mason - Unknown - Aerial Photography Principles and Applications {\_} EnvironmentalScience.html:html},
howpublished = {http://www.environmentalscience.org/principles-applications-aerial-photography. Date Accessed: 2016-11-20.},
title = {{Aerial Photography Principles and Applications {\_} EnvironmentalScience}},
url = {http://www.environmentalscience.org/principles-applications-aerial-photography},
urldate = {2016-11-20},
year = {2016}
}
@misc{Marr2016,
abstract = {It's all well and good to ask if androids dream of electric sheep, but science fact has evolved to a point where it's beginning to coincide with science fiction. No, we don't have autonomous androids struggling with existential crises — yet — but we are getting ever closer to what people tend to call “artificial intelligence.”},
author = {Marr, Bernard},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/A Short History of Machine Learning -- Every Manager Should Read - Tanjo.htm:htm},
howpublished = {https://tanjo.ai/contents/1084258},
pages = {10--13},
title = {{A Short History of Machine Learning}},
url = {https://tanjo.ai/contents/1084258},
urldate = {2016-11-07},
year = {2016}
}
@article{Dai2015,
abstract = {Semantic segmentation research has recently witnessed rapid progress, but many leading methods are unable to identify object instances. In this paper, we present Multi-task Network Cascades for instance-aware semantic segmentation. Our model consists of three networks, respectively differentiating instances, estimating masks, and categorizing objects. These networks form a cascaded structure, and are designed to share their convolutional features. We develop an algorithm for the nontrivial end-to-end training of this causal, cascaded structure. Our solution is a clean, single-step training framework and can be generalized to cascades that have more stages. We demonstrate state-of-the-art instance-aware semantic segmentation accuracy on PASCAL VOC. Meanwhile, our method takes only 360ms testing an image using VGG-16, which is two orders of magnitude faster than previous systems for this challenging problem. As a by product, our method also achieves compelling object detection results which surpass the competitive Fast/Faster R-CNN systems. The method described in this paper is the foundation of our submissions to the MS COCO 2015 segmentation competition, where we won the 1st place.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1512.04412},
author = {Dai, Jifeng and He, Kaiming and Sun, Jian},
doi = {10.1109/CVPR.2016.343},
eprint = {1512.04412},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/1512.04412v1.pdf:pdf},
isbn = {9781467388511},
issn = {10636919},
journal = {arXiv:1512.04412},
pages = {3150--3158},
title = {{Instance-aware Semantic Segmentation via Multi-task Network Cascades}},
year = {2015}
}
@article{Cvpr2016,
abstract = {Almost all of the current top-performing object detection networks employ region proposals to guide the search for object instances. State-of-the-art region proposal methods usually need several thousand proposals to get high recall, thus hurting the detection efficiency. Although the latest Region Proposal Network method gets promising detection accuracy with several hundred proposals, it still struggles in small-size object detection and precise localization (e.g., large IoU thresholds), mainly due to the coarseness of its feature maps. In this paper, we present a deep hierarchical network, namely HyperNet, for handling region proposal generation and object detection jointly. Our HyperNet is primarily based on an elaborately designed Hyper Feature which aggregates hierarchical feature maps first and then compresses them into a uniform space. The Hyper Features well incorporate deep but highly semantic, intermediate but really complementary, and shallow but naturally high-resolution features of the image, thus enabling us to construct HyperNet by sharing them both in generating proposals and detecting objects via an end-to-end joint training strategy. For the deep VGG16 model, our method achieves completely leading recall and state-of-the-art object detection accuracy on PASCAL VOC 2007 and 2012 using only 100 proposals per image. It runs with a speed of 5 fps (including all steps) on a GPU, thus having the potential for real-time processing.},
annote = {HyperNet

New best method for object detection?!

Faster and sometimes better than Fast R-CNN

- include the tabels in your paper :)},
archivePrefix = {arXiv},
arxivId = {1604.00600},
author = {Kong, Tao and Yao, Anbang and Chen, Yurong and Sun, Fuchun},
doi = {10.1109/CVPR.2016.98},
eprint = {1604.00600},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/Kong{\_}HyperNet{\_}Towards{\_}Accurate{\_}CVPR{\_}2016{\_}paper.pdf:pdf},
journal = {Cvpr},
pages = {845--853},
title = {{HyperNet : Towards Accurate Region Proposal Generation and Joint Object Detection}},
year = {2016}
}
@article{He2015,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learn- ing residual functions with reference to the layer inputs, in- stead of learning unreferenced functions. We provide com- prehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [41] but still having lower complex- ity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our ex- tremely deep representations, we obtain a 28{\%} relative im- provement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet local- ization, COCO detection, and COCO segmentation.},
annote = {ResNet
Cited: 676

God innledning p{\aa} deep learning

Svarer p{\aa} spm: Er det {\aa} f{\aa} et beder nettverk bare {\aa} legge p{\aa} flere layers?},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.3389/fpsyg.2013.00124},
eprint = {1512.03385},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/1512.03385v1.pdf:pdf},
isbn = {978-1-4673-6964-0},
issn = {1664-1078},
journal = {Arxiv.Org},
keywords = {deep learning,denoising auto-encoder,image denoising},
number = {3},
pages = {171--180},
pmid = {23554596},
title = {{Deep Residual Learning for Image Recognition}},
url = {http://arxiv.org/pdf/1512.03385v1.pdf},
volume = {7},
year = {2015}
}
@article{Hudjakov2011,
annote = {NULL},
author = {Hudjakov, R and Tamre, M},
doi = {10.3176/eng.2011.1.03},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/Ortophoto analysis for UGV long-range autonomous navigation .pdf:pdf},
issn = {1736-6038},
journal = {Estonian Journal of Engineering},
keywords = {convolutional neural net-,optical terrain classification,path planning,uav,ugv},
number = {1},
pages = {17},
title = {{Ortophoto analysis for UGV long-range autonomous navigation}},
volume = {17},
year = {2011}
}
@article{Seo2009,
author = {Seo, Young Woo and Ratliff, Nathan and Urmson, Chris},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/Self-Supervised Aerial Image Analysis for Extracting Parking Lot Structure.pdf:pdf},
isbn = {9781577354260},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {Robotics and Vision},
pages = {1837--1842},
title = {{Self-supervised aerial image analysis for extracting parking lot structure}},
year = {2009}
}
@article{McCulloch1943a,
abstract = {Because of the "all-or-none" character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {McCulloch, Warren S. and Pitts, Walter},
doi = {10.1007/BF02478259},
eprint = {arXiv:1011.1669v3},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/art{\%}3A10.1007{\%}2FBF02478259.pdf:pdf},
isbn = {0007-4985},
issn = {00074985},
journal = {The Bulletin of Mathematical Biophysics},
number = {4},
pages = {115--133},
pmid = {2185863},
title = {{A logical calculus of the ideas immanent in nervous activity}},
volume = {5},
year = {1943}
}
@article{Springenberg2015,
abstract = {Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the "deconvolution approach" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1412.6806},
author = {Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
eprint = {1412.6806},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/1412.6806v3.pdf:pdf},
isbn = {9781600066634},
journal = {Iclr},
pages = {1--14},
title = {{Striving for Simplicity: The All Convolutional Net}},
url = {http://arxiv.org/abs/1412.6806},
year = {2015}
}
@article{Levner,
abstract = {Automated image interpretation is an important task with nu- merous applications. Until recently, designing such systems required extensive subject matter and computer vision exper- tise resulting in poor cross-domain portability and expensive maintenance. Recently, a machine-learned system (ADORE) was successfully applied in an aerial image interpretation do- main. Subsequently, it was re-trained for another man-made object recognition task. In this paper we propose and imple- ment several extensions of ADORE addressing its primary limitations. These extensions enable the first successful ap- plication of this emerging AI technology to a natural image interpretation domain. The resulting system is shown to be robust with respect to noise in the training data, illumination, and camera angle variations as well as competitively adaptive with respect to novel images.},
annote = {NULL},
author = {Levner, Ilya and Bulitko, Vadim},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/Machine Learning for Adaptive Image Interpretation.pdf:pdf},
keywords = {Copyright {\textcopyright} 2004 American Association for Artifici,Markov decision models in vision,a fragment of an,a spruce,adaptive image interpretation,aerial image taken over,figure 1,forest,forest mapping,machine learning,mapping,markov decision models in,natural resource inventory,remote-sensing,vision},
mendeley-tags = {Markov decision models in vision,adaptive image interpretation,forest mapping,machine learning,natural resource inventory,remote-sensing},
pages = {870--876},
title = {{Machine Learning for Adaptive Image Interpretation}},
year = {2004}
}
@article{Hinton2012,
abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
archivePrefix = {arXiv},
arxivId = {1207.0580},
author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
doi = {arXiv:1207.0580},
eprint = {1207.0580},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/1207.0580.pdf:pdf},
isbn = {9781467394673},
issn = {9781467394673},
journal = {ArXiv e-prints},
pages = {1--18},
pmid = {1000104337},
title = {{Improving neural networks by preventing co-adaptation of feature detectors}},
url = {http://arxiv.org/abs/1207.0580},
year = {2012}
}
@article{Couprie2013,
abstract = {Scene labeling consists in labeling each pixel in an image with the category of the object it belongs to. We propose a method that uses a multiscale convolutional network trained from raw pixels to extract dense feature vectors that encode regions of multiple sizes centered on each pixel. The method alleviates the need for engineered features, and produces a powerful representation that captures texture, shape and contextual information.We report results using multiple post-processing methods to produce the final labeling. Among those, we propose a technique to automatically retrieve, from a pool of segmentation components, an optimal set of components that best explain the scene; these components are arbitrary, e.g. they can be taken from a segmentation tree, or from any family of over-segmentations. The system yields record accuracies on the Sift Flow Dataset (33 classes) and the Barcelona Dataset (170 classes) and near-record accuracy on Stanford Background Dataset (8 classes), while being an order of magnitude faster than competing approaches, producing a 320×240 image labeling in less than a second, including feature extraction.},
annote = {Mentioned in Stanford video},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Farabet, Cl{\'{e}}ment and Couprie, Camille and Najman, Laurent and LeCun, Yann},
doi = {10.1109/TPAMI.2012.231},
eprint = {arXiv:1011.1669v3},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/farabet-pami-13.pdf:pdf},
isbn = {0162-8828},
issn = {01628828},
journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
keywords = {Convolutional networks,deep learning,image classification,image segmentation,scene parsing},
number = {8},
pages = {1915--1929},
pmid = {23787344},
title = {{Learning Hierarchical Features for Scene Labeling}},
volume = {35},
year = {2013}
}
@techreport{Orstavik2016,
author = {{\O}rstavik, Mathilde},
file = {:Users/mathilde/skole/Prosjektoppgave/Mathilde {\O}rstavik, Prosjektoppgave.pdf:pdf},
institution = {NTNU},
title = {{The Use of Machine Learning to Extract Geographical Information from Remote Sensing Images}},
year = {2016}
}
@article{Nguyen2013,
abstract = {A satellite image classification method using Convolutional Neural Network (CNN) architecture is proposed in this paper. As a special case of deep learning, CNN classifies classes of images without any feature extraction step while other existing classification methods utilize rather complex feature extraction processes. Experiments on a set of satellite image data and the preliminary results show that the proposed classification method can be a promising alternative over existing feature extraction-based schemes in terms of classification accuracy and classification speed. {\textcopyright} 2013 AIP Publishing LLC.},
annote = {citet av 2 bare},
author = {Nguyen, Thao and Han, Jiho and Park, Dong-Chul},
doi = {10.1063/1.4825984},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/CNN satellite.pdf:pdf},
isbn = {9780735411845},
issn = {0094243X},
journal = {AIP Conference Proceedings},
keywords = {CNN,Classification,Deep Learning,Satellite Image},
number = {2013},
pages = {2237--2240},
title = {{Satellite image classification using convolutional learning}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84887555403{\&}partnerID=tZOtx3y1},
volume = {1558},
year = {2013}
}
@article{Zeiler2014,
abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
annote = {ZF net - winner 2013},
archivePrefix = {arXiv},
arxivId = {1311.2901},
author = {Zeiler, Matthew D. and Fergus, Rob},
doi = {10.1007/978-3-319-10590-1_53},
eprint = {1311.2901},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/1311.2901v3.pdf:pdf},
isbn = {978-3-319-10589-5},
issn = {978-3-319-10589-5},
journal = {Computer Vision–ECCV 2014},
pages = {818--833},
pmid = {26353135},
title = {{Visualizing and Understanding Convolutional Networks}},
url = {http://link.springer.com/10.1007/978-3-319-10590-1{\_}53{\%}5Cnhttp://arxiv.org/abs/1311.2901{\%}5Cnpapers3://publication/uuid/44feb4b1-873a-4443-8baa-1730ecd16291},
volume = {8689},
year = {2014}
}
@article{Yang2009,
abstract = {We propose an automatic approach to tree detection from aerial imagery. First a pixel-level classifier is trained to as- sign a {\{}tree, non-tree{\}} label to each pixel in an aerial image. The pixel-level classification is then refined by a partition- ing algorithm to a clean image segmentation of tree and non-tree regions. Based on the refined segmentation results, we adopt template matching followed by greedy selection to locate individual tree crowns. As training a pixel-level classifier requires manual gener- ation of ground-truth tree masks, we propose methods for automaticmodel and training data selection tominimize the manual work and scale the algorithmto the entire globe. We test the algorithm on thousands of production aerial images across different countries. We demonstrate high-quality tree detection results as well as good scalability of the proposed approach.},
author = {Yang, Lin and Wu, Xiaqing and Praun, Emil and Ma, Xiaoxu},
doi = {10.1145/1653771.1653792},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/35659Tree detection from aerial imagery.pdf:pdf},
isbn = {9781605586496},
journal = {Proceedings of the 17th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems - GIS '09},
number = {c},
pages = {131},
title = {{Tree detection from aerial imagery}},
url = {http://dl.acm.org/citation.cfm?id=1653771.1653792},
year = {2009}
}
@article{Russakovsky2015,
abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide detailed a analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1409.0575},
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
doi = {10.1007/s11263-015-0816-y},
eprint = {1409.0575},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/art{\%}3A10.1007{\%}2Fs11263-015-0816-y.pdf:pdf},
isbn = {0920-5691},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Benchmark,Dataset,Large-scale,Object detection,Object recognition},
number = {3},
pages = {211--252},
pmid = {16190471},
publisher = {Springer US},
title = {{ImageNet Large Scale Visual Recognition Challenge}},
url = {http://dx.doi.org/10.1007/s11263-015-0816-y},
volume = {115},
year = {2015}
}
@misc{Trezza,
author = {Trezza, Michael},
file = {:Users/mathilde/Library/Application Support/Mendeley Desktop/Downloaded/Trezza - 2016 - 4 Signs You Need to Start Business Process Reengineering - Lithyem.htm:htm},
howpublished = {http://lithyem.net/four-signs-you-need-to-start-business-process-reengineering/. Date Accessed: 2016-11-20.},
title = {{4 Signs You Need to Start Business Process Reengineering - Lithyem}},
url = {http://lithyem.net/four-signs-you-need-to-start-business-process-reengineering/},
urldate = {2016-11-20},
year = {2016}
}
@misc{Mat,
author = {MatConvNet-Team},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/MatConvNet.htm:htm},
howpublished = {http://www.vlfeat.org/matconvnet/. Date Acessed: 2016-12-17.},
title = {{MatConvNet}},
url = {file:///Users/mathilde/Skole/Prosjektoppgave/papers/MatConvNet.htm},
urldate = {2016-12-17},
year = {2016}
}
@article{Binford,
annote = {NULL},
author = {{MARCUS A. M ALOOF, Binford}, Thomas and Sage, Stephanie},
file = {:Users/mathilde/Library/Application Support/Mendeley Desktop/Downloaded/Binford, Sage - Unknown - 19980421 054(2).pdf:pdf},
pages = {23},
title = {{Improving Rooftop Detection in Aerial Images Through Machine Learning}},
year = {1998}
}
@misc{Mineault2014,
author = {Mineault, Patrick},
file = {:Users/mathilde/skole/papers/Adagrad – eliminating learning rates in stochastic gradient descent – xcorr{\_} comp neuro.htm:htm},
howpublished = {https://xcorr.net/2014/01/23/adagrad-eliminating-learning-rates-in-stochastic-gradient-descent/. Date Accessed: 26.05.2017},
title = {{Adagrad – eliminating learning rates in stochastic gradient descent}},
urldate = {2017-05-26},
year = {2014}
}
@misc{Li2016,
annote = {obj. det. slides},
author = {Li, Fei-fei and Karpathy, Andrej and Johnson, Justin},
booktitle = {Stanford Convolutional Neural Networks for Visual Recognition},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/winter1516{\_}lecture8.pdf:pdf},
howpublished = {http://cs231n.stanford.edu/slides/winter1516{\_}lecture8.pdf. Date Accessed: 14.03.2017},
pages = {1--90},
title = {{Lecture 08 : Spatial Localization and Detection}},
url = {http://cs231n.stanford.edu/slides/winter1516{\_}lecture8.pdf},
year = {2016}
}
@article{Qian1999,
annote = {Momentum},
author = {Qian, Ning},
file = {:Users/mathilde/skole/papers/1-s2.0-S0893608098001166-main.pdf:pdf},
isbn = {1212543521},
keywords = {critical damping,damped harmonic oscillator,gradient descent learning algorithm,learning rate,momentum,speed of convergence},
pages = {145--151},
title = {{On the momentum term in gradient descent learning algorithms}},
volume = {12},
year = {1999}
}
@book{Learning2013,
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/Deep Learning.htm:htm},
isbn = {9780262035613},
title = {{Deep Learning}},
url = {http://www.deeplearningbook.org/},
year = {2016}
}
@misc{Zhang,
author = {{Chiyuan Zhang}},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/Mocha.jl{\_} Deep Learning for Julia {\_} Parallel Forall.htm:htm},
howpublished = {https://devblogs.nvidia.com/parallelforall/mocha-jl-deep-learning-julia/},
title = {{Mocha.jl: Deep Learning for Julia}},
url = {https://devblogs.nvidia.com/parallelforall/mocha-jl-deep-learning-julia/},
urldate = {2016-11-24},
year = {2015}
}
@article{Petrozzo1994,
abstract = {AbeBooks.com: Successful Reengineering: (full book description) Van Nostrand Reinhold Co., New York, NY, 1994. 1st Edition Fine/Fine, Hard Cover, w/Dust Jacket. Size=6"x9", 336pp(Index). Diagrams {\&} Figures. Clean, tight {\&} bright. NO ink names, bookplates, DJ tears etc. 99{\%} OF OUR BOOKS ARE SHIPPED IN CUSTOM BOXES, WE ALWAYS PACK WITH GREAT CARE!},
author = {Petrozzo, Daniel P. and Stepper, John C.},
file = {:Users/mathilde/Library/Application Support/Mendeley Desktop/Downloaded/Petrozzo, Stepper - 1994 - Successful Reengineering.pdf:pdf},
isbn = {0471286028},
journal = {New York : Van Nostrand Reinhold},
pages = {336},
title = {{Successful Reengineering}},
url = {https://www.goodreads.com/book/show/5667796-successful-reengineering},
year = {1994}
}
@misc{He2017a,
annote = {Github issue for mask-rcnn, written by Kaiming He author of the paper},
author = {He, Kaiming},
booktitle = {GitHub issue},
file = {:Users/mathilde/skole/papers/clarification {\textperiodcentered} Issue {\#}3 {\textperiodcentered} CharlesShang{\_}FastMaskRCNN.htm:htm},
howpublished = {https://github.com/CharlesShang /FastMaskRCNN/issues/3. Date Accessed: 2017-05-11},
title = {{Mask R-CNN clarification}},
url = {https://github.com/CharlesShang/FastMaskRCNN/issues/3},
urldate = {2017-05-11},
year = {2017}
}
@misc{ISPRS2015,
author = {ISPRS},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/The ISPRS data set collection.htm:htm},
howpublished = {http://www.isprs.org/data/. Date Accessed: 17.12.2016.},
title = {{The ISPRS data set collection}},
url = {http://www.isprs.org/data/},
urldate = {2016-12-17},
year = {2015}
}
@article{Zhang2013,
abstract = {Recognizing aerial image categories is useful for scene annotation and surveillance. Local features have been demonstrated to be robust to image transformations, including occlusions and clutters. However, the geometric property of an aerial image (i.e., the topology and relative displacement of local features), which is key to discriminating aerial image categories, cannot be effectively represented by state-of-the-art generic visual descriptors. To solve this problem, we propose a recognition model that mines graphlets from aerial images, where graphlets are small connected subgraphs reflecting both the geometric property and color/texture distribution of an aerial image. More specifically, each aerial image is decomposed into a set of basic components (e.g., road and playground) and a region adjacency graph (RAG) is accordingly constructed to model their spatial interactions. Aerial image categories recog- nition can subsequently be casted as RAG-to-RAG matching. Based on graph theory, RAG-to-RAG matching is conducted by comparing all their respective graphlets. Because the number of graphlets is huge, we derive a manifold embedding algorithm to measure different-sized graphlets, after which we select graphlets that have highly discriminative and low redundancy topologies. Through quantizing the selected graphlets from each aerial image into a feature vector, we use support vector machine to discriminate aerial image categories. Experimental results indicate that our method outperforms several state-of-the-art object/scene recognition models, and the visualized graphlets indicate that the discriminative patterns are discovered by our proposed approach.},
author = {Zhang, Luming and Han, Yahong and Yang, Yi and Song, Mingli and Yan, Shuicheng and Tian, Qi},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/1241(mye sitert).pdf:pdf},
keywords = {Aerial image category,discrimination,graphlets,redundancy.,selection,topologies},
mendeley-tags = {Aerial image category,discrimination,graphlets,redundancy.,selection,topologies},
number = {12},
pages = {5071--5084},
title = {{Discovering Discriminative Graphlets for Aerial Image Categories Recognition}},
volume = {22},
year = {2013}
}
@article{Glorot2010,
abstract = {Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We ﬁrst observe the inﬂuence of the non-linear activations functions. We ﬁnd that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we ﬁnd that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We ﬁnd that a new non-linearity that saturates less can often be beneﬁcial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difﬁcult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
annote = {Proposed Xavier weight initialization method},
author = {Glorot, Xavier and Bengio, Yoshua},
doi = {10.1.1.207.2059},
file = {:Users/mathilde/skole/papers/AISTATS2010{\_}Glorot.pdf:pdf},
isbn = {9781937284275},
issn = {15324435},
journal = {Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS)},
pages = {249--256},
title = {{Understanding the difficulty of training deep feedforward neural networks}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/AISTATS2010{\_}GlorotB10.pdf},
volume = {9},
year = {2010}
}
@article{Herold2004,
abstract = {We investigate the spectral complexity and unique spectral characteristics of urban environments using a comprehensive regional field spectral library of more than 4500 individual spectra. Spectral properties of urban surface materials are presented and interpreted, and their separability is systematically analyzed using the Bhattacharyya distance (B-distance) as a quantitative measure of spectral discrimination. We find considerable spectral confusion between urban land cover types (i.e. specific roof and road types) but also show the potential of fine spectral-resolution remote sensing for detailed mapping of urban materials and their condition based on their spectral signal. An evaluation of the most suitable wavelengths for separation of urban land cover identified specific spectral features that provided the best separation. There is a strong indication that current multispectral systems, including IKONOS and LANDSAT ETM+, provide only marginal abilities to resolve these important features and are limited for urban land-cover mapping. {\textcopyright} 2004 Elsevier Inc. All rights reserved.},
author = {Herold, Martin and Roberts, Dar A. and Gardner, Margaret E. and Dennison, Philip E.},
doi = {10.1016/j.rse.2004.02.013},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/rse04{\_}heroldetal.pdf:pdf},
isbn = {1805893149},
issn = {00344257},
journal = {Remote Sensing of Environment},
keywords = {AVIRIS,Bhattacharyya distance,Spectral library,Urban area remote sensing},
number = {3-4},
pages = {304--319},
pmid = {678},
title = {{Spectrometry for urban area remote sensing - Development and analysis of a spectral library from 350 to 2400 nm}},
volume = {91},
year = {2004}
}
@misc{Li2016a,
author = {Li, Fei-fei and Karpathy, Andrej and Johnson, Justin},
booktitle = {Stanford Convolutional Neural Networks for Visual Recognition},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/winter1516{\_}lecture13.pdf:pdf},
howpublished = {http://cs231n.stanford.edu/slides/winter1516{\_}lecture13.pdf. Date Accessed: 2016-12-03},
pages = {1--133},
title = {{Lecture 13 : Segmentation and Attention}},
url = {http://cs231n.stanford.edu/slides/winter1516{\_}lecture13.pdf},
urldate = {2016-12-03},
year = {2016}
}
@article{McCulloch1943,
abstract = {Because of the 'all-or-none' character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behaviour of every net can be described in these terms, with the addition of more complicated logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which nehaves under the other and gives the same results, algthough perhaps not in the same time. Various applications of the calculus are discussed.},
author = {McCulloch, W S and Pitts, W},
doi = {10.1007/BF02478259},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/mcp.pdf:pdf},
isbn = {0007-4985},
issn = {0007-4985},
journal = {Bulletin of Mathematical Biophysics},
keywords = {McCulloch and Pitts,neuron},
pages = {115--133},
pmid = {2185863},
title = {{A Logical Calculus of the Idea Immanent in Nervous Activity}},
url = {http://www.cse.chalmers.se/{~}coquand/AUTOMATA/mcp.pdf},
volume = {5},
year = {1943}
}
@misc{imageNet,
annote = {ImageNet},
author = {Deng, J. and Dong, W. and Socher, R. and Li, L. and Li, K. and Fei-Fei, L.},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/ILSVRC2016.htm:htm},
howpublished = {http://image-net.org/. Date Accessed: 2016-11-24.},
title = {{ImageNet}},
url = {http://image-net.org/},
urldate = {2016-11-24},
year = {2009}
}
@article{Chen2014a,
abstract = {Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called "semantic image segmentation"). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our "DeepLab" system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6{\%} IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the 'hole' algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1412.7062},
author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
eprint = {1412.7062},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/1606.00915.pdf:pdf},
isbn = {9783901608353},
journal = {Iclr},
pages = {1--14},
title = {{Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs}},
url = {http://arxiv.org/abs/1412.7062},
year = {2014}
}
@article{MohammedElAmin2016,
author = {{Mohammed El Amin}, Arabi and Liu, Qingjie and Wang, Yunhong},
doi = {10.1117/12.2243798},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/Convolutional Neural Network Features Based Change Detection in Satellite Images.pdf:pdf},
keywords = {cd,change detection,cnn,convolutional neural network,high resolution remote sensing,hrrs},
number = {figure 1},
pages = {100110W},
title = {{Convolutional neural network features based change detection in satellite images}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2243798},
volume = {10011},
year = {2016}
}
@article{Mnih2013,
abstract = {Information extracted from aerial photographs has found applications in a wide range of areas including urban planning, crop and forest management, disaster relief, and climate modeling. At present, much of the extraction is still performed by human experts, making the process slow, costly, and error prone. The goal of this thesis is to develop methods for automatically extracting the locations of objects such as roads, buildings, and trees directly from aerial images. We investigate the use of machine learning methods trained on aligned aerial images and possibly outdated maps for labeling the pixels of an aerial image with se- mantic labels. We show how deep neural networks implemented on modern GPUs can be used to efficiently learn highly discriminative image features. We then introduce new loss functions for training neural networks that are partially robust to incom- plete and poorly registered target maps. Finally, we propose two ways of improving the predictions of our system by introducing structure into the outputs of the neural networks. We evaluate our system on the largest and most-challenging road and building detection datasets considered in the literature and show that it works reliably under a wide variety of conditions. Furthermore, we are releasing the first large-scale road and building detection datasets to the public in order to facilitate future comparisons with other methods.},
author = {Mnih, Volodymyr},
file = {:Users/mathilde/Library/Application Support/Mendeley Desktop/Downloaded/Mnih - 2013 - Machine Learning for Aerial Image Labeling.pdf:pdf},
isbn = {9780494721308},
journal = {PhD Thesis},
pages = {109},
title = {{Machine Learning for Aerial Image Labeling}},
year = {2013}
}
@article{Yaegashi2010,
abstract = {In this paper, we treat with generic object recognition for geotagged images. As a recognition method for geotagged photos, we have already proposed exploiting aerial photos around geotag places as additional image features for visual recognition of geotagged photos. In the previous work, to fuse two kinds of features, we just concatenate them. Instead, in this paper, we introduce Multiple Kernel Learning (MKL) to integrate both features of photos and aerial images. MKL can estimate the contribution weights to integrate both kinds of features. In the experiments, we confirmed effectiveness of usage of aerial photos for recognition of geotagged photos, and we evaluated the weights of both features estimated by MKL for eighteen concepts.},
author = {Yaegashi, Keita and Yanai, Keiji},
doi = {10.1109/ICPR.2010.800},
file = {:Users/mathilde/Library/Application Support/Mendeley Desktop/Downloaded/Yaegashi, Yanai - 2010 - Geotagged photo recognition using corresponding aerial photos with multiple kernel learning.pdf:pdf},
isbn = {9780769541099},
issn = {10514651},
journal = {Proceedings - International Conference on Pattern Recognition},
number = {1},
pages = {3272--3275},
pmid = {5597498},
title = {{Geotagged photo recognition using corresponding aerial photos with multiple kernel learning}},
year = {2010}
}
@misc{Digital-globe,
annote = {NULL},
author = {DigitalGlobe},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/GBDX Platform - DigitalGlobe.htm:htm},
howpublished = {https://developer.digitalglobe.com/gbdx/. Date Accessed: 2016-12-17.},
title = {{DigitalGlobe}},
url = {https://developer.digitalglobe.com/gbdx/},
urldate = {2016-12-17},
year = {2016}
}
@article{Girshick2014,
abstract = {Can a large convolutional neural network trained for whole-image classification on ImageNet be coaxed into detecting objects in PASCAL? We show that the answer is yes, and that the resulting system is simple, scalable, and boosts mean average precision, relative to the venerable deformable part model, by more than 40{\%} (achieving a final mAP of 48{\%} on VOC 2007). Our framework combines powerful computer vision techniques for generating bottom-up region proposals with recent advances in learning high-capacity convolutional neural networks. We call the resulting system R-CNN: Regions with CNN features. The same framework is also competitive with state-of-the-art semantic segmentation methods, demonstrating its flexibility. Beyond these results, we execute a battery of experiments that provide insight into what the network learns to represent, revealing a rich hierarchy of discriminative and often semantically meaningful features.},
annote = {R-CNN !!},
archivePrefix = {arXiv},
arxivId = {1311.2524},
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
doi = {10.1109/CVPR.2014.81},
eprint = {1311.2524},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/r-cnn-cvpr.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {580--587},
pmid = {26656583},
title = {{Rich feature hierarchies for accurate object detection and semantic segmentation}},
year = {2014}
}
@article{Yu2015,
abstract = {State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy.},
archivePrefix = {arXiv},
arxivId = {1511.07122},
author = {Yu, Fisher and Koltun, Vladlen},
doi = {10.16373/j.cnki.ahr.150049},
eprint = {1511.07122},
file = {:Users/mathilde/skole/papers/1511.07122.pdf:pdf},
isbn = {0894-0282},
issn = {00237205},
pmid = {22352717},
title = {{Multi-Scale Context Aggregation by Dilated Convolutions}},
url = {http://arxiv.org/abs/1511.07122},
year = {2015}
}
@article{Zhang2016,
abstract = {Due to the recent advances in satellite sensors, a large amount of high-resolution remote sensing images is now being obtained each day. How to automatically recognize and analyze scenes from these satellite images effectively and efficiently has become a big challenge in the remote sensing field. Recently, a lot of work in scene classification has been proposed, focusing on deep neural networks, which learn hierarchical internal feature representations from image data sets and produce state-of-the-art performance. However, most methods, including the traditional shallow methods and deep neural networks, only concentrate on training a single model. Meanwhile, neural network ensembles have proved to be a powerful and practical tool for a number of different predictive tasks. Can we find a way to combine different deep neural networks effectively and efficiently for scene classification? In this paper, we propose a gradient boosting random convolutional network (GBRCN) framework for scene classification, which can effectively combine many deep neural networks. As far as we know, this is the first time that a deep ensemble framework has been proposed for scene classification. Moreover, in the experiments, the proposed method was applied to two challenging high-resolution data sets: 1) the UC Merced data set containing 21 different aerial scene categories with a submeter resolution and 2) a Sydney data set containing eight land-use categories with a 1.0-m spatial resolution. The proposed GBRCN framework outperformed the state-of-the-art methods with the UC Merced data set, including the traditional single convolutional network approach. For the Sydney data set, the proposed method again obtained the best accuracy, demonstrating that the proposed framework can provide more accurate classification results than the state-of-the-art methods.},
annote = {NULL},
author = {Zhang, Fan and Du, Bo and Zhang, Liangpei},
doi = {10.1109/TGRS.2015.2488681},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/Scene Classification via a Gradient Boosting Random Convolutional Network Framework.pdf:pdf},
issn = {01962892},
journal = {IEEE Transactions on Geoscience and Remote Sensing},
keywords = {Convolutional networks (CNets),Gradient boosting machine (GBM),Scene classification},
number = {3},
pages = {1793--1802},
title = {{Scene classification via a gradient boosting random convolutional network framework}},
volume = {54},
year = {2016}
}
@article{Ishii2015,
annote = {NULL},
author = {Ishii, Tomohiro and Nakamura, Ryosuke and Nakada, Hidemoto and Mochizuki, Yoshihiko and Ishikawa, Hiroshi},
doi = {10.1109/MVA.2015.7153200},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/Surface Object Recognition with CNN and SVM in Landsat 8 Images.pdf:pdf},
isbn = {9784901122153},
journal = {Proceedings of the 14th IAPR International Conference on Machine Vision Applications, MVA 2015},
keywords = {Remote Sensing,Object Recognition},
pages = {341--344},
title = {{Surface object recognition with CNN and SVM in Landsat 8 images}},
year = {2015}
}
@article{Ioffe2015,
abstract = {{Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch{\}}. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9{\%} top-5 validation error (and 4.8{\%} test error), exceeding the accuracy of human raters.},
annote = {Batch normalization paper - this was reffered to by Andrej},
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Ioffe, Sergey and Szegedy, Christian},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1502.03167},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/1502.03167v3.pdf:pdf},
isbn = {9780874216561},
issn = {0717-6163},
journal = {arXiv:1502.03167},
pages = {1--11},
pmid = {15003161},
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
url = {http://arxiv.org/abs/1502.03167},
year = {2015}
}
@article{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different “thinned” networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets},
archivePrefix = {arXiv},
arxivId = {1102.4807},
author = {Srivastava, Nitish and Hinton, Geoffrey E. and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
doi = {10.1214/12-AOS1000},
eprint = {1102.4807},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/JMLRdropout.pdf:pdf},
isbn = {1532-4435},
issn = {15337928},
journal = {Journal of Machine Learning Research (JMLR)},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
title = {{Dropout : A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}
@book{Macukow2016,
abstract = {Modelling and simulating discrete production processes issues were discussed in this paper. Electric scooter production line at the start-up stage was analysed. Requirements towards its efficiency were change before the line going on-line. Simulation model used production plans, drafted prior to starting production. It considered among other adding machinery, increase production capacity, process of production and transport, transport routes, layout, material flow, human resources and means of transport. The manufacturing process was designed to use KANBAN. Hence simulation models including various alternative solutions of Technical and Organisational Production Set-up were used to verify whether 12 minute cycle time was achieved. Modification to the production system included among other interchangeability of workstations and operators. Consequently bottle necks in the production process were eliminated. The project discussed in this paper proves feasibility and validity of combining simulation of production systems with Lean Manufacturing tools. {\textcopyright} 2013 IFIP International Federation for Information Processing.},
author = {Macukow, Bohdan},
doi = {10.1007/978-3-642-40925-7},
editor = {Saeed, Khalid and Wladyslaw, Homenda},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/chp{\%}3A10.1007{\%}2F978-3-319-45378-1{\_}1.pdf:pdf},
isbn = {978-3-642-40924-0},
issn = {03029743},
keywords = {cycle time,production system,simulation of discrete processes},
pages = {476--486},
publisher = {Springer International Publishing},
title = {{Neural Networks – State of Art, Brief History, Basic Models and Architecture}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84885200850{\&}partnerID=tZOtx3y1},
volume = {8104},
year = {2016}
}
@misc{Waterloo2014,
author = {{City of Waterloo}},
file = {:Users/mathilde/Library/Application Support/Mendeley Desktop/Downloaded/City of Waterloo - 2014 - Aerial Imagery.htm:htm},
howpublished = {http://opendata.city-of-waterloo.opendata.arcgis.com/datasets/2fefc0a997914ba083beb6a39cded4f1. Date Accessed: 2016-11-20.},
title = {{Aerial Imagery}},
url = {http://opendata.city-of-waterloo.opendata.arcgis.com/datasets/2fefc0a997914ba083beb6a39cded4f1},
urldate = {2016-11-20},
year = {2014}
}
@article{Krizhevsky2012,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSRVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state of the art. The neural network, which has 60 million paramters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolutional operation. To reduce overfitting in the fully-connected layers, we employed a recently-developed method called 'dropout' that proved to be effective. We also entered a variant of the model in the ILSVRC-2012 competition and achievd a top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
doi = {http://dx.doi.org/10.1016/j.protcy.2014.09.007},
eprint = {1102.0183},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/imagenet-classification-with-deep-convolutional-neural-networks.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances In Neural Information Processing Systems},
pages = {1--9},
pmid = {7491034},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
year = {2012}
}
@article{Wilcox2013,
author = {Wilcox, Catherine and Woon, Wei Lee and Aung, Zeyar},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/DNA-2013-03.pdf:pdf},
number = {December},
title = {{Applications of machine learning in environmental engineering}},
year = {2013}
}
@misc{Bollinger2017a,
author = {Bollinger, Drew},
howpublished = {https://www.developmentseed.org/blog/2017/01/30/machine-learning-learnings/ Date accessed: 2017-05-07},
title = {{Open Source Machine Learning}},
url = {https://www.developmentseed.org/blog/2017/01/30/machine-learning-learnings/},
urldate = {2017-05-07},
year = {2017}
}
@article{Li2016c,
abstract = {We present the first fully convolutional end-to-end solution for instance-aware semantic segmentation task. It inherits all the merits of FCNs for semantic segmentation and instance mask proposal. It performs instance mask prediction and classification jointly. The underlying convolutional representation is fully shared between the two sub-tasks, as well as between all regions of interest. The proposed network is highly integrated and achieves state-of-the-art performance in both accuracy and efficiency. It wins the COCO 2016 segmentation competition by a large margin. The code would be released at $\backslash$url{\{}https://github.com/daijifeng001/TA-FCN{\}}.},
annote = {Instance segmentation},
archivePrefix = {arXiv},
arxivId = {1611.07709},
author = {Li, Yi and Qi, Haozhi and Dai, Jifeng and Ji, Xiangyang and Wei, Yichen},
eprint = {1611.07709},
file = {:Users/mathilde/skole/papers/1611.07709.pdf:pdf},
journal = {arXiv preprint},
title = {{Fully Convolutional Instance-aware Semantic Segmentation}},
url = {http://arxiv.org/abs/1611.07709},
year = {2016}
}
@techreport{FKB,
author = {Kartverket},
file = {:Users/mathilde/Library/Application Support/Mendeley Desktop/Downloaded/Kartverket - 2013 - Produktspesifikasjon for FKB.pdf:pdf},
number = {4.02},
pages = {1--93},
title = {{Produktspesifikasjon for FKB}},
year = {2013}
}
@misc{Li2015a,
author = {Li, Fei-Fei and Karpathy, Andrej},
booktitle = {Stanford Convolutional Neural Networks for Visual Recognition},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/winter1516{\_}lecture7.pdf:pdf},
howpublished = {http://cs231n.stanford.edu/slides/winter1516{\_}lecture7.pdf. Date Accessed: 2016-11-28.},
pages = {1--89},
title = {{Convolutional Neural Networks (Lecture 7)}},
url = {http://cs231n.stanford.edu/slides/winter1516{\_}lecture7.pdf},
urldate = {2016-11-28},
year = {2015}
}
@article{Mnih2010,
abstract = {Reliably extracting information fromaerial imagery is a difficult prob- lemwith many practical applications. One specific case of this problem is the task of automatically detecting roads. This task is a difficult vision problem because of occlusions, shadows, and a wide variety of non-road objects. Despite 30 years of work on automatic road detection, no automatic or semi-automatic road detec- tion system is currently on the market and no published method has been shown to work reliably on large datasets of urban imagery. We propose detecting roads using a neural network with millions of trainable weights which looks at a much larger context than was used in previous attempts at learning the task. The net- work is trained on massive amounts of data using a consumer GPU.We demon- strate that predictive performance can be substantially improved by initializing the feature detectors using recently developed unsupervised learning methods as well as by taking advantage of the local spatial coherence of the output labels.We show that our method works reliably on two challenging urban datasets that are an order ofmagnitude larger than what was used to evaluate previous approaches. 1},
author = {Mnih, Volodymyr and Hinton, Geoffrey E.},
doi = {10.1007/978-3-642-15567-3_16},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/Learning to Detect Roads in High-Resolution Aerial Images.pdf:pdf},
isbn = {3642155669},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 6},
pages = {210--223},
title = {{Learning to detect roads in high-resolution aerial images}},
volume = {6316 LNCS},
year = {2010}
}
@article{Long2017,
author = {Long, Yang and Gong, Yiping and Xiao, Zhifeng and Liu, Qing},
doi = {10.1109/TGRS.2016.2645610},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/07827088.pdf:pdf},
issn = {0196-2892},
journal = {IEEE Transactions on Geoscience and Remote Sensing},
pages = {1--13},
title = {{Accurate Object Localization in Remote Sensing Images Based on Convolutional Neural Networks}},
url = {http://ieeexplore.ieee.org/document/7827088/},
year = {2017}
}
@misc{Lun2016,
author = {Lun, Tseng Kuan},
booktitle = {GitHub},
file = {:Users/mathilde/Library/Application Support/Mendeley Desktop/Downloaded/Lun - 2016 - TensorFlow SegNet.html:html},
howpublished = {https://github.com/tkuanlun350/Tensorflow-SegNet. Date Accessed: 2017-05-12},
title = {{TensorFlow SegNet}},
url = {https://github.com/tkuanlun350/Tensorflow-SegNet},
urldate = {2017-05-12},
year = {2016}
}
@article{Zhou2016,
abstract = {Scene parsing, or recognizing and segmenting objects and stuff in an image, is one of the key problems in computer vision. Despite the community's efforts in data collection, there are still few image datasets covering a wide range of scenes and object categories with dense and detailed annotations for scene parsing. In this paper, we introduce and analyze the ADE20K dataset, spanning diverse annotations of scenes, objects, parts of objects, and in some cases even parts of parts. A generic network design called Cascade Segmentation Module is then proposed to enable the segmentation networks to parse a scene into stuff, objects, and object parts in a cascade. We evaluate the proposed module integrated within two existing semantic segmentation networks, yielding significant improvements for scene parsing. We further show that the scene parsing networks trained on ADE20K can be applied to a wide variety of scenes and objects.},
annote = {Cascade segNet

State of the art within segmentation I think. at least around august 2016},
archivePrefix = {arXiv},
arxivId = {1608.05442},
author = {Zhou, Bolei and Zhao, Hang and Puig, Xavier and Fidler, Sanja and Barriuso, Adela and Torralba, Antonio},
eprint = {1608.05442},
file = {:Users/mathilde/Library/Application Support/Mendeley Desktop/Downloaded/Zhou et al. - 2016 - Semantic Understanding of Scenes through the ADE20K Dataset.pdf:pdf},
journal = {arXiv},
title = {{Semantic Understanding of Scenes through the ADE20K Dataset}},
url = {http://arxiv.org/abs/1608.05442},
year = {2016}
}
@article{Langkvist2016,
abstract = {The availability of high-resolution remote sensing (HRRS) data has opened up the possibility for new interesting applications, such as per-pixel classification of individual objects in greater detail. This paper shows how a convolutional neural network (CNN) can be applied to multispectral orthoimagery and a digital surface model (DSM) of a small city for a full, fast and accurate per-pixel classification. The predicted low-level pixel classes are then used to improve the high-level segmentation. Various design choices of the CNN architecture are evaluated and analyzed. The investigated land area is fully manually labeled into five categories (vegetation, ground, roads, buildings and water), and the classification accuracy is compared to other per-pixel classification works on other land areas that have a similar choice of categories. The results of the full classification and segmentation on selected segments of the map show that CNNs are a viable tool for solving both the segmentation and object recognition task for remote sensing data.},
annote = {Cited 9 ganger

Referer til papers som har f{\aa}tt gode resultater p{\aa} spesifikke omr{\aa}der: 

object recognition [17,18], object detection [19], scene parsing [20,21] and scene classification [22–24]

Sliter med {\aa} skj{\o}nne om denne paperen er bra eller ikke. Den sier at cnn med en layer er best, men det stemmer jo absolutt ikke med annen research?! Whaaat},
author = {L{\"{a}}ngkvist, Martin and Kiselev, Andrey and Alirezaie, Marjan and Loutfi, Amy},
doi = {10.3390/rs8040329},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/remotesensing-08-00329.pdf:pdf},
issn = {20724292},
journal = {Remote Sensing},
keywords = {Convolutional neural network,Orthoimagery,Per-pixel classification,Region merging,Remote sensing,Segmentation},
number = {4},
title = {{Classification and segmentation of satellite orthoimagery using convolutional neural networks}},
volume = {8},
year = {2016}
}
@article{Bayoudh2013,
abstract = {The number of satellites and remote sensing sensors devoted to earth observation becomes increasingly high, providing more and more data and especially images. In the same time the access to such a data and to the tools to process them has been considerably improved. In the presence of such data flow - and regarding the necessity to follow up and predict environmental and societal changes in highly dynamic socio-environmental contexts - we need automatic image interpretation methods. This could be accomplished by exploring some strengths of artificial intelligence. Our main idea consists in inducing classification rules that explicitly take into account structural knowledge, using Aleph, an Inductive Logic Programming (ILP) system.We applied our proposed methodology to three land cover/use maps of the French Guiana littoral. One hundred and forty six classification rules were induced for the 39 land-cover classes of the maps. These rules are expressed in first order logic language which make them intelligible and interpretable by non-experts. A ten-fold cross validation gave average values for classification accuracy, specificity and sensibility equal to, respectively, 98.82 {\%}, 99.65{\%} and 70{\%}. The proposed methodology could be valuably exploited to automatically classify new objects and/or help operators using object-based classification procedures.},
annote = {NULL},
author = {Bayoudh, Meriam and Roux, Emmanuel and Nock, Richard and Richard, G and Bayoudh, Meriam and Roux, Emmanuel and Nock, Richard and Automatic, G Richard},
file = {:Users/mathilde/Library/Application Support/Mendeley Desktop/Downloaded/Bayoudh et al. - 2013 - Automatic learning of structural knowledge from geographic information for updating land cover maps.pdf:pdf},
keywords = {Geographic Information System (GIS),Inductive Logic Programming (ILP),Land cover/use maps,Machine learning,Remote sensing},
mendeley-tags = {Geographic Information System (GIS),Inductive Logic Programming (ILP),Land cover/use maps,Machine learning,Remote sensing},
title = {{Automatic learning of structural knowledge from geographic information for updating land cover maps}},
year = {2013}
}
@article{Nogueira2016,
abstract = {We present an analysis of three possible strategies for exploiting the power of existing convolutional neural networks (ConvNets or CNNs) in different scenarios from the ones they were trained: full training, fine tuning, and using ConvNets as feature extractors. In many applications, especially including remote sensing, it is not feasible to fully design and train a new ConvNet, as this usually requires a considerable amount of labeled data and demands high computational costs. Therefore, it is important to understand how to better use existing ConvNets. We perform experiments with six popular ConvNets using three remote sensing datasets. We also compare ConvNets in each strategy with existing descriptors and with state-of-the-art baselines. Results point that fine tuning tends to be the best performing strategy. In fact, using the features from the fine-tuned ConvNet with linear SVM obtains the best results. We also achieved state-of-the-art results for the three datasets used.},
annote = {Ser p{\aa} hvordan utnytte eksiterende nett:
- Pre trained
- Fine-tuning

Tregner flere eksempler? Sjekk ref 15, 17, 18, 19

cited by 5},
archivePrefix = {arXiv},
arxivId = {1602.01517},
author = {Nogueira, Keiller and Penatti, Ot{\'{a}}vio A B and dos Santos, Jefersson A.},
doi = {10.1016/j.patcog.2016.07.001},
eprint = {1602.01517},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/1602.01517v1.pdf:pdf},
isbn = {5531340958},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Aerial scenes,Convolutional neural networks,Deep learning,Feature extraction,Fine-tune,Hyperspectral images,Remote sensing},
title = {{Towards better exploiting convolutional neural networks for remote sensing scene classification}},
year = {2016}
}
@article{Zeiler2012,
archivePrefix = {arXiv},
arxivId = {arXiv:1212.5701v1},
author = {Zeiler, Matthew D.},
eprint = {arXiv:1212.5701v1},
file = {:Users/mathilde/skole/papers/1212.5701.pdf:pdf},
title = {{ADADELTA: AN ADAPTIVE LEARNING RATE METHOD}},
year = {2012}
}
@misc{DanKuster,
author = {{Dan Kuster}},
file = {:Users/mathilde/skole/papers/The Good, Bad, {\&} Ugly of TensorFlow.htm:htm},
howpublished = {https://indico.io/blog/the-good-bad-ugly-of-tensorflow/. Date Accessed: 2017-06-12},
title = {{The Good, Bad, {\&} Ugly of TensorFlow}},
url = {https://indico.io/blog/the-good-bad-ugly-of-tensorflow/},
urldate = {2017-06-12},
year = {2016}
}
@article{Sensing2016,
annote = {cited by 6},
author = {Marmanis, D and Wegner, J D and Galliani, S and Schindler, K and Datcu, M and Stilla, U},
doi = {10.5194/isprsannals-III-3-473-2016},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/SEMANTIC SEGMENTATION OF AERIAL IMAGES WITH AN ENSEMBLE OF CNNS.pdf:pdf},
number = {July},
pages = {12--19},
title = {{Semantic segmentation of aerial images with an ensemble of cnns}},
volume = {III},
year = {2016}
}
@article{Zhang2016a,
annote = {Good as inspiration for writing the conlusion!!!

Book},
author = {Zhang, L. and Zhang, L. and Kumar, V.},
doi = {10.1155/2016/7954154},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/07486259.pdf:pdf},
issn = {16877268},
journal = {IEEE Geoscience and Remote Sensing Magazine},
number = {june},
pages = {18},
title = {{Deep learning for Remote Sensing Data}},
year = {2016}
}
@article{Hammer1990,
author = {Hammer, Michael},
file = {:Users/mathilde/Library/Application Support/Mendeley Desktop/Downloaded/Hammer - 1990 - Reengineering Work, Don't Automate, Obliterate.pdf:pdf},
journal = {Harvard Business Review},
title = {{Reengineering Work, Don't Automate, Obliterate}},
year = {1990}
}
@article{Castelluccio2015,
abstract = {We explore the use of convolutional neural networks for the semantic classification of remote sensing scenes. Two recently proposed architectures, CaffeNet and GoogLeNet, are adopted, with three different learning modalities. Besides conventional training from scratch, we resort to pre-trained networks that are only fine-tuned on the target data, so as to avoid overfitting problems and reduce design time. Experiments on two remote sensing datasets, with markedly different characteristics, testify on the effectiveness and wide applicability of the proposed solution, which guarantees a significant performance improvement over all state-of-the-art references.},
archivePrefix = {arXiv},
arxivId = {1508.00092},
author = {Castelluccio, Marco and Poggi, Giovanni and Sansone, Carlo and Verdoliva, Luisa},
eprint = {1508.00092},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/1508.00092.pdf:pdf},
journal = {arXiv preprint arXiv:1508.00092},
pages = {1--11},
title = {{Land Use Classification in Remote Sensing Images by Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1508.00092},
year = {2015}
}
@article{Bellman2012,
annote = {NULL},
author = {Bellman, Chris J},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/A{\_}Machine{\_}Learning{\_}Approach{\_}to{\_}Building{\_}Recognitio.pdf:pdf},
keywords = {building detection,classification,learning,multiresolution},
number = {May},
title = {{A Machine Learning Approach to Building Recognition in Aerial Photographs}},
year = {2012}
}
@article{Hu2016,
abstract = {Aerial image categorization plays an indispensable role in remote sensing and artificial intelligence. In this paper, we propose a new aerial image categorization framework, focusing on organizing the local patches of each aerial image into multiple discriminative subgraphs. These meaningful subgraphs reflect both the geometric property and the color distribution of an aerial image. First, each aerial image is decomposed into a collection of regions in terms of their color intensities. Thereby region connected graph (RCG), which models the connection between the spatial neighboring regions, is constructed to encode the spatial context of an aerial image. Second, a novel subgraph mining technique is adopted to discover the frequent structures in the RCGs constructed from the training aerial images. Thereafter, a set of refined structures is selected among the frequent ones toward being highly discriminative and low redundant. Lastly, given a new aerial image, its sub-RCGs corresponding to the refined structures are extracted. They are further quantized into a discriminative vector for SVM classification. Thorough experimental results validate the effectiveness of the proposed method. In addition, the visualized mined subgraphs show that the discriminative topologies of each aerial image are discovered.},
archivePrefix = {arXiv},
arxivId = {1410.2188},
author = {Hu, Yuxing and Nie, Liqiang},
doi = {10.1016/j.jvcir.2015.04.004},
eprint = {1410.2188},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/An aerial image recognition framework using discrimination and redundancy quality measure.pdf:pdf},
issn = {10959076},
journal = {Journal of Visual Communication and Image Representation},
keywords = {Aerial image,Categorization,Data mining,Discriminative,Framework,Image recognition,Quality measure,Subgraph},
pages = {53--62},
publisher = {Elsevier Inc.},
title = {{An aerial image recognition framework using discrimination and redundancy quality measure}},
url = {http://dx.doi.org/10.1016/j.jvcir.2015.04.004},
volume = {37},
year = {2016}
}
@misc{Li2016b,
author = {Li, Fei-fei and Karpathy, Andrej and Johnson, Justin},
booktitle = {Stanford Convolutional Neural Networks for Visual Recognition},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/winter1516{\_}lecture12.pdf:pdf},
howpublished = {http://cs231n.stanford.edu/slides/winter1516{\_}lecture12.pdf. Date Accessed: 14.03.2017},
pages = {1--177},
title = {{Lecture 12 : Software Packages}},
url = {http://cs231n.stanford.edu/slides/winter1516{\_}lecture12.pdf},
year = {2016}
}
@article{Lin2016,
abstract = {We show how the success of deep learning depends not only on mathematics but also on physics: although well-known mathematical theorems guarantee that neural networks can approximate arbitrary functions well, the class of functions of practical interest can be approximated through "cheap learning" with exponentially fewer parameters than generic ones, because they have simplifying properties tracing back to the laws of physics. The exceptional simplicity of physics-based functions hinges on properties such as symmetry, locality, compositionality and polynomial log-probability, and we explore how these properties translate into exceptionally simple neural networks approximating both natural phenomena such as images and abstract representations thereof such as drawings. We further argue that when the statistical process generating the data is of a certain hierarchical form prevalent in physics and machine-learning, a deep neural network can be more efficient than a shallow one. We formalize these claims using information theory and discuss the relation to renormalization group procedures. Various "no-flattening theorems" show when these efficient deep networks cannot be accurately approximated by shallow ones without efficiency loss - even for linear networks.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1608.08225},
author = {Lin, Henry W. and Tegmark, Max},
eprint = {1608.08225},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/Why does deep and cheap learning work so well?.pdf:pdf},
pages = {14},
title = {{Why does deep and cheap learning work so well?}},
url = {http://arxiv.org/abs/1608.08225},
volume = {02139},
year = {2016}
}
@article{Bengio2012,
abstract = {Learning algorithms related to artificial neural net- works and in particular for Deep Learning may seem to involve many bells and whistles, called hyper- parameters. This chapter is meant as a practical guide with recommendations for some of the most commonly used hyper-parameters, in particular in the context of learning algorithms based on back- propagated gradient and gradient-based optimiza- tion. It also discusses how to deal with the fact that more interesting results can be obtained when allow- ing one to adjust many hyper-parameters. Overall, it describes elements of the practice used to successfully and efficiently train and debug large-scale and often deep multi-layer neural networks. It closes with open questions about the training difficulties observedwith deeper architectures. 1},
annote = {Hyperparameter explanation and suggestions},
archivePrefix = {arXiv},
arxivId = {1206.5533},
author = {Bengio, Yoshua},
doi = {10.1007/978-3-642-35289-8-26},
eprint = {1206.5533},
file = {:Users/mathilde/Library/Application Support/Mendeley Desktop/Downloaded/Bengio - 2012 - Practical recommendations for gradient-based training of deep architectures.pdf:pdf},
isbn = {9783642352881},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {437--478},
pmid = {25497547},
title = {{Practical recommendations for gradient-based training of deep architectures}},
volume = {7700 LECTU},
year = {2012}
}
@article{Felzenszwalb2008,
abstract = {This paper describes a discriminatively trained, multiscale, deformable$\backslash$npart model for object detection. Our system achieves a two-fold improvement$\backslash$nin average precision over the best performance in the 2006 PASCAL$\backslash$nperson detection challenge. It also outperforms the best results$\backslash$nin the 2007 challenge in ten out of twenty categories. The system$\backslash$nrelies heavily on deformable parts. While deformable part models$\backslash$nhave become quite popular, their value had not been demonstrated$\backslash$non difficult benchmarks such as the PASCAL challenge. Our system$\backslash$nalso relies heavily on new methods for discriminative training. We$\backslash$ncombine a margin-sensitive approach for data mining hard negative$\backslash$nexamples with a formalism we call latent SVM. A latent SVM, like$\backslash$na hidden CRF, leads to a non-convex training problem. However, a$\backslash$nlatent SVM is semi-convex and the training problem becomes convex$\backslash$nonce latent information is specified for the positive examples. We$\backslash$nbelieve that our training methods will eventually make possible the$\backslash$neffective use of more latent information such as hierarchical (grammar)$\backslash$nmodels and models involving latent three dimensional pose. View full$\backslash$nabstract},
annote = {Deformable Parts Model
- state of the art for object detection bbox before CNN came},
author = {Felzenszwalb, Pedro and McAllester, David and Ramanan, Deva},
doi = {10.1109/CVPR.2008.4587597},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/04587597.pdf:pdf},
isbn = {9781424422432},
issn = {1063-6919},
journal = {Cvpr},
pages = {1--8},
pmid = {20634557},
title = {{A Discriminatively Trained, Multiscaled, Deformable Part Model}},
url = {http://ieeexplore.ieee.org/xpls/abs{\%}7B{\_}{\%}7Dall.jsp?arnumber=4587597{\$}{\%}5C{\$}npapers2://publication/doi/10.1109/CVPR.2008.4587597},
year = {2008}
}
@book{Jensen2014,
author = {Jensen, John R.},
edition = {Second Edi},
publisher = {Pearson Education},
title = {{Remote Sensing of the Enviroment: An Earth Resource Perspective}},
year = {2014}
}
@misc{Vistage,
author = {Vistage},
file = {:Users/mathilde/Library/Application Support/Mendeley Desktop/Downloaded/Vistage - Unknown - Eight steps to a successful grant award.htm:htm},
howpublished = {http://www.vistage.com/resource/eight-steps-successful-reengineering-effort/},
title = {{Eight steps to a successful grant award}},
url = {http://www.vistage.com/resource/eight-steps-successful-reengineering-effort/}
}
@misc{Bollinger2017,
author = {Bollinger, Drew},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/Open Source Machine Learning — Development Seed.htm:htm},
howpublished = {https://www.developmentseed.org/blog/2017/01/30/machine-learning-learnings/},
title = {{Open Source Machine Learning — Development Seed}},
url = {https://www.developmentseed.org/blog/2017/01/30/machine-learning-learnings/},
urldate = {2017-01-24},
year = {2017}
}
@article{Jia2014,
abstract = {Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep mod-els efficiently on commodity architectures. Caffe fits indus-try and internet-scale media needs by CUDA GPU computa-tion, processing over 40 million images a day on a single K40 or Titan GPU (≈ 2.5 ms per image). By separating model representation from actual implementation, Caffe allows ex-perimentation and seamless switching among platforms for ease of development and deployment from prototyping ma-chines to cloud environments. Caffe is maintained and developed by the Berkeley Vi-sion and Learning Center (BVLC) with the help of an ac-tive community of contributors on GitHub. It powers on-going research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.},
annote = {Paper about Caffe},
archivePrefix = {arXiv},
arxivId = {1408.5093},
author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
doi = {10.1145/2647868.2654889},
eprint = {1408.5093},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/p675-jia.pdf:pdf},
isbn = {9781450330633},
issn = {9781450330633},
journal = {Proceedings of the ACM International Conference on Multimedia - MM '14},
keywords = {Computer Vision,D22 [Software Engineering],Design,Experimentation Keywords Open Source,I51 [Pattern Recognition],Machine Learning * Corresponding Authors,Neural Networks,Parallel Computation,[Applications–Computer vi-sion],[Design Tools and Techniques–Software libraries],[Models–Neural Nets] General Terms Algorithms},
pages = {675--678},
pmid = {18267787},
title = {{Caffe}},
url = {http://dl.acm.org/citation.cfm?doid=2647868.2654889},
year = {2014}
}
@misc{Shang2017,
author = {Shang, Charles},
file = {:Users/mathilde/skole/papers/CharlesShang{\_}FastMaskRCNN{\_} Mask RCNN in TensorFlow.htm:htm},
howpublished = {https://github.com/CharlesShang/FastMaskRCNN. Date Accessed: 2017-05-11},
title = {{Mask R-CNN TensorFlow implementation}},
url = {https://github.com/CharlesShang/FastMaskRCNN},
urldate = {2017-05-11},
year = {2017}
}
@misc{GISGeography2016,
author = {GISGeography},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/Spectral Signature Cheatsheet - Spectral Bands in Remote Sensing - GIS Geography.htm:htm},
howpublished = {http://gisgeography.com/spectral-signature/. Date Accessed: 2016-11-25.},
title = {{Spectral Signature Cheatsheet - Spectral Bands in Remote Sensing - GIS Geography}},
url = {http://gisgeography.com/spectral-signature/},
urldate = {2016-11-25},
year = {2016}
}
@article{Lin2014,
abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
archivePrefix = {arXiv},
arxivId = {arXiv:1405.0312v1},
author = {Lin, Tsung Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'{a}}r, Piotr and Zitnick, C. Lawrence},
doi = {10.1007/978-3-319-10602-1_48},
eprint = {arXiv:1405.0312v1},
file = {:Users/mathilde/Library/Application Support/Mendeley Desktop/Downloaded/Lin et al. - 2014 - Microsoft COCO Common objects in context.pdf:pdf},
isbn = {978-3-319-10601-4},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 5},
pages = {740--755},
title = {{Microsoft COCO: Common objects in context}},
volume = {8693 LNCS},
year = {2014}
}
@article{Jacobs2014,
author = {Jacobs, Alexander D and Steiner, Robert R},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/Jacobs, Detection.pdf:pdf},
pages = {1--5},
title = {{Detection of the Duquenois – Levine chromophore in a marijuana sample}},
volume = {239},
year = {2014}
}
@article{Long2015,
abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20{\%} relative improvement to 62.2{\%} mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.},
annote = {Very popular paper! Said by stanford dude},
archivePrefix = {arXiv},
arxivId = {1411.4038},
author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
doi = {10.1109/CVPR.2015.7298965},
eprint = {1411.4038},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/Fully Convolutional Networks for Semantic Segmentation.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {3431--3440},
pmid = {16190471},
title = {{Fully convolutional networks for semantic segmentation}},
volume = {07-12-June},
year = {2015}
}
@article{tensorflow,
abstract = {TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. TensorFlow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous "parameter server" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with particularly strong support for training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model in contrast to existing systems, and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.},
archivePrefix = {arXiv},
arxivId = {1605.08695},
author = {Abadi, Mart{\'{i}}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G. and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
eprint = {1605.08695},
file = {:Users/mathilde/Library/Application Support/Mendeley Desktop/Downloaded/Abadi et al. - 2016 - TensorFlow A system for large-scale machine learning.pdf:pdf},
isbn = {978-1-931971-33-1},
journal = {Google Brain},
pages = {18},
title = {{TensorFlow: A system for large-scale machine learning}},
url = {http://arxiv.org/abs/1605.08695},
year = {2016}
}
@book{Hammer1993,
author = {{Michael Hammer}, James Champy},
title = {{Reengineering the corporation: A manifesto for business revolution}},
year = {1993}
}
@article{He2017,
abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without tricks, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code will be made available.},
annote = {Tips fra Jan erik!

Tydeligvis state of the art per 21.03.2017},
archivePrefix = {arXiv},
arxivId = {1703.06870},
author = {He, Kaiming and Gkioxari, Georgia and Doll{\'{a}}r, Piotr and Girshick, Ross},
eprint = {1703.06870},
file = {:Users/mathilde/Library/Application Support/Mendeley Desktop/Downloaded/He et al. - 2017 - Mask R-CNN.pdf:pdf},
title = {{Mask R-CNN}},
year = {2017}
}
@article{Ren2015a,
abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [7] and Fast R-CNN [5] have reduced the running time of these detection networks, exposing region pro-posal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolu-tional features. For the very deep VGG-16 model [18], our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2{\%} mAP) and 2012 (70.4{\%} mAP) using 300 proposals per image. The code will be released.},
annote = {Fast R-CNN
Best method for object detection, specifically finding the bbox. It is explained in stanford class!},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.01497v1},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
doi = {10.1016/j.nima.2015.05.028},
eprint = {arXiv:1506.01497v1},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf:pdf},
isbn = {0162-8828 VO - PP},
issn = {01689002},
journal = {Nips},
pages = {1--10},
pmid = {27295650},
title = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}},
year = {2015}
}
@article{Mishkin2015,
abstract = {Layer-sequential unit-variance (LSUV) initialization - a simple strategy for weight initialization for deep net learning - is proposed. The strategy proceeds from the first to the final layer, normalizing the variance of the output of each layer to be equal to one. We show that with the strategy, learning of very deep nets via standard stochastic gradient descent is at least as fast as the complex schemes proposed specifically for very deep nets such as FitNets (Romero et al. (2015)) and Highway (Srivastava et al. (2015)). Performance that is state-of-the-art, or very close to it, is achieved on the MNIST, CIFAR, ImageNet datasets.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1511.06422},
author = {Mishkin, Dmytro and Matas, Jiri},
doi = {10.1016/0898-1221(96)87329-9},
eprint = {1511.06422},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/1511.06422v7.pdf:pdf},
isbn = {0262560992},
issn = {87550229},
journal = {Iclr},
keywords = {Initialization,Optimization},
pages = {1--8},
pmid = {21595383},
title = {{All you need is a good init}},
url = {http://arxiv.org/abs/1511.06422},
year = {2015}
}
@article{Pinheiro2014,
abstract = {Scene parsing is a technique that consist on giving a label to all pixels in an image according to the class they belong to. To ensure a good visual coherence and a high class accuracy, it is essential for a scene parser to capture image long range dependencies. In a feed-forward architecture, this can be simply achieved by considering a sufficiently large input context patch, around each pixel to be labeled. We propose an approach consisting of a recurrent convolutional neural network which allows us to consider a large input context, while limiting the capacity of the model. Contrary to most standard approaches, our method does not rely on any segmentation methods, nor any task-specific features. The system is trained in an end-to-end manner over raw pixels, and models complex spatial dependencies with low inference cost. As the context size increases with the built-in recurrence, the system identifies and corrects its own errors. Our approach yields state-of-the-art performance on both the Stanford Background Dataset and the SIFT Flow Dataset, while remaining very fast at test time.},
archivePrefix = {arXiv},
arxivId = {1306.2795},
author = {Pinheiro, Pho and Collobert, Ronan},
eprint = {1306.2795},
file = {:Users/mathilde/Skole/Prosjektoppgave/papers/pinheiro14.pdf:pdf},
isbn = {9781634393973},
journal = {Proceedings of The 31st International Conference {\ldots}},
number = {June},
pages = {82--90},
title = {{Recurrent convolutional neural networks for scene labeling}},
url = {http://infoscience.epfl.ch/record/192577/files/Pinheiro{\_}Idiap-RR-41-2013.pdf{\%}5Cnhttp://jmlr.org/proceedings/papers/v32/pinheiro14.html},
volume = {32},
year = {2014}
}
